\mbox{}\\
\vspace{8cm}

This chapter is a reproduction of the following publication:

B. C. L. van der Putten, C. I. Mendes, B. M. Talbot, J. de Korne-Elenbaas, R. Mamede, P. Vila-Cerqueira, L. P. Coelho, C. A. Gulvik, L. S. Katz and The ASM NGS 2020 Hackathon participants. Software testing in microbial bioinformatics: a call to action. Microbial Genomics, Volume 8, Issue 3, March 2020. DOI: \url{https://doi.org/ 10.1099/mgen.0.000790}

TO DO 

\cleardoublepage 

\begin{center}
\large
\textbf{Software testing in microbial bioinformatics: a call to action}
\end{center}

Boas C. L. van der Putten$^{1,2,*}$, 
Catarina I. Mendes$^{3,*}$,
Brooke M. Talbot$^{4}$, 
Jolinda de Korne-Elenbaas$^{1,5}$, 
R. Mamede$^{3}$, 
P. Vila-Cerqueira$^{3}$, 
L. P. Coelho$^{6,7}$, 
C. A. Gulvik$^{8}$, 
L. S. Katz$^{9,10}$ and The ASM NGS 2020 Hackathon participants

$^1$Department of Medical Microbiology, Amsterdam UMC, University of Amsterdam, the Netherlands

$^2$Department of Global Health, Amsterdam Institute for Global Health and Development, Amsterdam UMC, University of Amsterdam, the Netherlandss

$^3$Instituto de Microbiologia, Instituto de Medicina Molecular, Faculdade de Medicina, Universidade de Lisboa, Lisboa, Portugal 

$^4$Department of Biological and Biomedical Sciences, Emory University, Atlanta, GA, USA

$^5$Department of Infectious Diseases, Public Health Laboratory, Public Health Service of Amsterdam, the Netherlands

$^6$Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, PR China

$^7$Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, PR China

$^8$Bacterial Special Pathogens Branch, Division of High-Consequence Pathogens and Pathology, Centers for Disease Control and Prevention, Atlanta, GA, USA

$^9$Center for Food Safety, University of Georgia, Griffin, GA, USA

$^{10}$Enteric Diseases Laboratory Branch, Division of Foodborne, Waterborne, and Environmental Diseases, Centers for Disease Control and Prevention, Atlanta, GA, USA

$^*$Contributed equally

\section{Abstract} \label{sec:ch8_abstract}

Computational algorithms have become an essential component of research, with great efforts by the scientific community to raise standards on development and distribution of code. Despite these efforts, sustainability and reproducibility are major issues since continued validation through software testing is still not a widely adopted practice. Here, we report seven recommendations that help researchers implement software testing in microbial bioinformatics. We have developed these recommendations based on our experience from a collaborative hackathon organised prior to the American Society for Microbiology Next Generation Sequencing (ASM NGS) 2020 conference. We also present a repository hosting examples and guidelines for testing, available from \url{https://github.com/microbinfie-hackathon2020/CSIS}.


\section{Impact Statement}

In the field of microbial bioinformatics, good software engineering practises are not yet widely adopted. Many microbial bioinformaticians start out as (micro)biologists and subsequently learn how to code. Without abundant formal training, a lot of education about good software engineering practices comes down to an exchange of information within the microbial bioinformatics community. This paper serves as a resource that could help microbial bioinformaticians get started with software testing if they have not had formal training.

\section{Background}

Computational algorithms, software, and workflows have enhanced the breadth and depth of microbiological research and expanded the capacity of infectious disease surveillance in public health practice. Scientists now have a wealth of bioinformatic tools for addressing pertinent questions quickly and keeping pace with the availability of larger and more complex biological datasets. Despite these advances, we are finding ourselves in a crisis of computational reproducibility \cite{stodden_empirical_2018}.

Modern software engineering advocates reliable software testing standards and best practices. Different approaches are employed: from unit testing to system testing \cite{krafczyk_scientific_2019}, going from testing every individual component to testing a tool as a whole (Fig. \ref{fig:chap8_figure1}). The extent of testing is a balance between the resources available and increasing sustainability and reproducibility. Continuous Integration (CI), where code changes are frequently integrated and assertion of the new code’s correctness before integration is often automatedly performed through tests, provides a robust approach for ensuring the reproducibility of scientific results without requiring human interaction. Comprehensive testing of scientific software might prevent computational errors which subsequently lead to erroneous results and retractions \cite{chang_retraction_2006,hall_retraction_2007}. However, the role of testing extends beyond that, as it also provides a way to measure software coverage, and therefore its robustness, allowing for reported issues to be converted into testable actions (regression tests), and the expansion and refactoring of existing code without compromising its function.

\begin{figure*}[]
\centering
\includegraphics[width=\textwidth]{figures/chapter 8/mgen000790-f1.png}
\caption{Testing strategies. (a) White-box vs. black-box testing. In white-box testing, the tester knows the underlying code and structure of the software, where the tester does not know this in black-box testing. Note that this distinction is not strictly dichotomous and is considered less useful nowadays (b) Unit vs. integration vs. system testing. When software comprises several modules, it is possible to test each single module (unit testing), groups of related modules (integration testing) or all modules (system testing). Note that the terms white-box testing and unit testing are sometimes used interchangeably but relate to different concepts}
\label{fig:chap8_figure1}
\end{figure*}

Software testing among peers across fields aligns with previous efforts of hackathons to create a more unified and informed bioinformatics software community \cite{busby_closing_2016}. In this context, we hosted a cooperative hackathon prior to the ASM NGS conference in 2020, demonstrating that the microbial bioinformatics community can contribute to software sustainability using a collaborative platform (Table \ref{}). From this experience, we would like to propose collaborative software testing as an opportunity to continuously engage software users, developers, and students to unify scientific work across domains. We have outlined the following recommendations for ensuring software sustainability through testing and offer a repository of automated test knowledge and examples at the Code Safety Inspection Service (CSIS) repository on GitHub (\url{https://github.com/microbinfie-hackathon2020/CSIS}).

\section{Recommendations}

Based on our experiences from the ASM NGS 2020 hackathon, we developed seven recommendations that can be followed during software development.

\subsection{Establish software needs and testing goals}

Manually testing the functionality of a tool is feasible in early development but can become laborious as the software matures. Developers may establish software needs and testing goals during the planning and designing stages to ensure an efficient testing structure. Table \ref{tab:ch8_table1} provides an overview of testing methodologies and can serve as a guide to developers that aim to implement testing practises. A minimal test set could address the validation of core components or the programme as a whole (system testing) and gradually progress toward verification of key functions which can accommodate code changes over time (unit testing, Figure \ref{fig:chap8_figure1}). Ideally, testing should be implemented from the early stages of software development (test-driven development). Defining the scope of testing is important before developing tests. For pipeline development, testing of each individual component can be laborious and can be expedited if those components already implement testing of their own. Testing of the pipeline itself should take priority.

\begin{table}[]
\centering
\caption{Overview of testing approaches. Software testing can be separated into three types: installation, functionality and destructive. Each component is described, followed by an example on a real-life application on Software X, a hypothetical nucleotide sequence annotation tool}
\label{tab:ch8_table1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Name &
  Description &
  Example \\ \midrule
\multicolumn{3}{c}{\textbf{Installation testing: can the software be invoked on different setups?}} \\ \midrule
Installation testing &
  Can the software be installed on different platforms? &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X can be installed \\ using apt-get, pip, conda and from source.\end{tabular} \\
Configuration testing &
  With which dependencies can the software be used? &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X can be used \\ with different versions of blast+.\end{tabular} \\
Implementation testing &
  \begin{tabular}[c]{@{}l@{}}Do different implementations work \\ similarly enough?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X works the same \\ between the standalone and webserver versions.\end{tabular} \\
Compatibility testing &
  \begin{tabular}[c]{@{}l@{}}Are newer versions compatible with \\ previous input/output?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X can be used with \\ older versions of the UniProtKB database.\end{tabular} \\
Static testing &
  Is the source code syntactically correct? &
  \begin{tabular}[c]{@{}l@{}}Check whether all opening braces have \\ corresponding closing braces or whether \\ code is indented correctly in Software X.\end{tabular} \\ \midrule
\multicolumn{3}{c}{\textbf{Standard functionality testing: does the software do what it should in daily use?}} \\ \midrule
Use case testing &
  \begin{tabular}[c]{@{}l@{}}Can the software do what it is supposed to \\ do regularly?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X can annotate different \\ FASTA files: with spaces in the header, \\ without a header, an empty file, with spaces in \\ the sequence, with unknown characters in the \\ sequences, et cetera.\end{tabular} \\
Workflow testing &
  \begin{tabular}[c]{@{}l@{}}Can the software successfully traverse each \\ path in the analysis?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X works in different \\ modes (using fast mode or using one \\ dependency over the other).\end{tabular} \\
Sanity testing &
  Can the software be invoked without errors? &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X works correctly without \\ flags, or when checking dependencies or \\ displaying help info.\end{tabular} \\ \midrule
\multicolumn{3}{c}{\textbf{Destructive testing: what makes the software fail?}} \\ \midrule
Mutation testing &
  \begin{tabular}[c]{@{}l@{}}How do the current tests handle harmful \\ alterations to the software?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether changing a single addition to a \\ subtraction within Software X causes \\ the test suite to fail.\end{tabular} \\
Load testing &
  At what input size does the software fail? &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X can annotate a small \\ plasmid (10 kbp), a medium-size genome \\ (2 Mbp) or an unrealistically large \\ genome for a prokaryote (1 Gbp).\end{tabular} \\
Fault injection &
  \begin{tabular}[c]{@{}l@{}}Does the software fail if faults are introduced \\ and how is this handled?\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Test whether Software X fails if nonsense \\ functions are introduced in the gene calling code.\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Input test files: the good, the bad, and the ugly}

When testing, it is important to include test files with known expected outcomes for a successful run. However, it is equally important to include files or other inputs on which the tool is expected to fail. For example, some tools should recognize and report an empty input file or a wrong input format. Therefore, the test dataset should be small enough to be easily deployed (see recommendation #4) but as large as necessary to cover all intended test cases. Data provenance should be disclosed, either if it’s from real data or originated in silico. Typically, a small test data is packaged with the software. Examples of valid and invalid file formats are available through the BioJulia project (\url{https://github.com/BioJulia/BioFmtSpecimens}). The nf-core project (\url{https://nf-co.re/}) provides a repository with test data for a myriad of cases (\url{https://github.com/nf-core/test-datasets}).

\subsection{Use an established framework to implement testing}

Understanding the test workflow can not only ensure continued software development but also the integrity of the project for developers and users. Testing frameworks improve test development and efficiency. Examples include unittest (\url{https://docs.python.org/3/library/unittest.html}) or pytest (\url{https://docs.pytest.org/en/stable/}) for Python, and testthat (\url{https://testthat.r-lib.org/}) for R, testing interfaces such as TAP (\url{http://testanything.org/}), or built-in test attributes such as in Rust. Although many tests can be implemented using a combination of frameworks, personal preferences (e.g. amount of boilerplate code required) might drive your choice. Additionally, in Github Actions the formulas of each test block can be explicitly stated using the standardised and easy-to-follow YAML (\url{https://yaml.org/}, Fig. \ref{}, available in the online version of this article), already adopted by most continuous integration platforms (recommendation \#4). For containerised software, testing considerations differ slightly and have been covered previously by Gruening et al. (2019) \cite{gruening_recommendations_2019}.

\subsection{Testing is good, automated testing is better}

When designing tests, planning for automation saves development time. Whether your tests are small or comprehensive, automatic triggering of tests will help reduce your workload. Many platforms trigger tests automatically based on a set of user-defined conditions. Platforms such as GitHub Actions (\url{https://github.com/features/actions}) and GitLab CI (\url{https://about.gitlab.com/stages-devops-lifecycle/continuous-integration}) offer straightforward automated testing of code seamlessly upon deployment. A typical workflow, consisting of a minimal testing framework (see recommendation \#1 and \#3) and a small test dataset (see recommendation \#2), can then be directly integrated within your project hosted on a version control system, such as GitHub (\url{https://github.com/}), and directly integrated with a continuous integration provider, such as GitHub Actions in GitHub. Testing considerations for containerised software has been covered previously by Gruening et al. (2019) \cite{gruening_recommendations_2019}.

\subsection{Ensure portability by testing on several platforms}

The result of an automated test in the context of one computational workspace does not ensure the same result will be obtained in a different setup. It is important to ensure your software can be installed and used across supported platforms. One way to ensure this is to test on different environments, with varying dependency versions (e.g. multiple Python versions, instead of only the most recent one). Developers can gain increased benefits of testing if tests are run on different setups automatically (see recommendation #4 and Fig. \ref{}).

\subsection{Showcase the tests}

For prospective users, it is good to know whether you have tested your software and, if so, which tests you have included. This can be done by displaying a badge \cite{trockman_adding_2018} (see \url{https://github.com/microbinfie-hackathon2020/CSIS/blob/main/README.md#example-software-testing}), or linking to your defined testing strategy e.g. a GitHub Actions YAML, (see recommendation \#2, Fig. \ref{}). Documenting the testing goal and process enables end-users to easily check tool functionality and the level of testing \cite{karimzadeh_top_2018}.

It may be helpful to contact the authors, directly or through issues in the code repository, whose software you have tested to share successful outcomes or if you encountered abnormal behaviour or component failures. An external perspective can be useful to find bugs that the authors are unaware of. A set of issue templates for various situations is available in the CSIS repository on GitHub (\url{https://github.com/microbinfie-hackathon2020/CSIS/tree/main/templates}).

\subsection{Encourage others to test your software}

Software testing can be crowdsourced, as showcased by the ASM NGS 2020 hackathon. Software suites such as Pangolin (\url{https://github.com/cov-lineages/pangolin}) \cite{otoole_assignment_2021} and chewBBACA (\url{https://github.com/B-UMMI/chewBBACA}) \cite{silva_chewbbaca_nodate} have implemented automated testing developed during the hackathon. For developers, crowdsourcing offers the benefits of fresh eyes on your software. Feedback and contributions from users can expedite the implementation of software testing practices. It also contributes to software sustainability by creating community buy-in, which ultimately helps the software maintainers keep pace with dependency changes and identify current user needs.

\section{Conclusions}
 
Testing is a critical aspect of scientific software development, but automated software testing remains underused in scientific software. In this hackathon, we demonstrated the usefulness of testing and developed a set of recommendations that should improve the development of tests. We also demonstrated the feasibility of producing test suites for already-established microbial bioinformatics software (Table S1).

 
\section{Funding information}
 
C.I.M. was supported by the Fundação para a Ciência e Tecnologia (grant SFRH/BD/129483/2017). L.P.C. was partially supported by Shanghai Municipal Science and Technology Major Project (2018SHZDZX01) and ZJLab. R. M. was supported by the Fundação para a Ciência e Tecnologia (grant 2020.08493.BD).

 
\section{Acknowledgements}
 
The findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention (CDC). The mention of company names or products does not constitute an endorsement by the CDC.

 
\section{Author contributions}
 
In addition to the authors, the following participants were responsible for automating tests for bioinformatic tools and contributing a community resource for identifying software that can pass unit tests, available at \url{https://github.com/microbinfie-hackathon2020/CSIS}. Participants are listed alphabetically: Áine O’Toole, Amit Yadav, Justin Payne, Mario Ramirez, Peter van Heusden, Robert A. Petit III, Verity Hill, Yvette Unoarumhi.

 
\section{Conflicts of interest}
 
The authors declare that there are no conflicts of interest.

\section{Supplemental Material}

\begin{figure*}[]
\centering
\includegraphics[width=\textwidth]{figures/chapter 8/mgen000790-f1.png}
\caption{Testing strategies. (a) White-box vs. black-box testing. In white-box testing, the tester knows the underlying code and structure of the software, where the tester does not know this in black-box testing. Note that this distinction is not strictly dichotomous and is considered less useful nowadays (b) Unit vs. integration vs. system testing. When software comprises several modules, it is possible to test each single module (unit testing), groups of related modules (integration testing) or all modules (system testing). Note that the terms white-box testing and unit testing are sometimes used interchangeably but relate to different concepts}
\label{fig:chap8_figure1}
\end{figure*}