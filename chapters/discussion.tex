\mbox{}\\
\vspace{8cm}

The rise of life lost due to microbial pathogens, particularly when associated with the surge of \ac{AMR}, poses a major threat to human health around the world. Optimising the diagnostic process is crucial in managing infectious diseases \citep{vos_global_2020}. Currently, the golden standard for clinical microbiology are culture, antimicrobial susceptibility testing, \ac{PCR}, including syndromic multiplex testing, and serology. Sequencing, when applied, is usually limited to 16S for prokaryotic pathogen identification \citep{greninger_challenge_2018}. 

In this thesis, we have evaluated the use of bioinformatics methods for the analysis of metagenomic data to allow the rapid identification, virulence analysis and antimicrobial susceptibility prediction of pathogens with clinical relevance, from both diagnostic and a surveillance settings. With the widespread use and continuous development of sequencing technologies, bioinformatics has become a cornerstone in modern clinical microbiology. As mentioned previously, the lack of golden standards severely hinders the applicability of bioinformatic methods, particularly in \ac{SMg} \citep{carrico_primer_2018, couto_critical_2018, angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017}. 

Metagenomics, and in particular \ac{SMg}, has emerged as a promising approach for diagnosis from clinical samples and surveillance of organisms of interest from the environment \citep{loman_culture-independent_2013, rossen__2018, schuele_future_2021, chiu_clinical_2019}. A single metagenomics analysis has the potential to detect common, rare and novel pathogens, and provides a broad overall picture of the microbial content present in a sample. Despite this, is often unclear whether a given detected microorganism is a contaminant, coloniser or \textit{bona fide} pathogen, and the lack of golden standards remains one of the biggest challenges when applying these methods in clinical microbiology for diagnosis. 

Several limitations have been identified that, in its current form, curb the applicability of these methods in both clinical and public health microbiology. 

\subsection{Limitation to the application of metagenomics methodologies}

\subsubsection{Limitations of sequencing technologies}

While the application of genomics in clinical microbiology has been increasing, the translation of genetic information remains challenging. Recent advances in DNA sequencing technologies have expanded their application as a diagnostic tool, but limitations still prevail. After over a quarter of a century of development and maturation, several technologies are available to be used both in research and in the clinic, from the first generation DNA chain termination sequencing to third generation long-read sequencing. Despite this, the main benefit if current clinical microbiology testing paradigm in comparison with genomic approaches is that it allows for cost-effective negative results \citep{greninger_challenge_2018}. Regardless, the initial capital cost of setting up a genomics capable facility for the use of metagenomics are considerable, having been estimated at around one million United States dollars \citep{greninger_challenge_2018}.

First generation sequencing technologies requires the input \ac{DNA} to consist of a pure population of sequences, as each molecule will contribute to the final eletropherogram, as it is a superposition of all of the input molecules \citep{hagemann_overview_2015}. As such, it cannot be applied to metagenomic methodologies. 

Second generation sequencing so far represent the most popular technology applied in metagenomics \citep{rossen_practical_2018, loman_twenty_2015, loman_high-throughput_2012}. Second-generation methods require library preparation and an enrichment or amplification step \citep{hagemann_overview_2015}, a time-consuming, bias inducing procedure that is propagated to the resulting data. Another limitation is the size of the outputted sequences that, despite the massive throughput of some of the machines available, requiring a very small \ac{DNA} input load to produce up to billions of sequences \citep{loman_twenty_2015}, ranges from from 45 to 300 bases in length \citep{loman_performance_2012, ari_next-generation_2016}. This is simply not enough to not only transverse the most repetitive genomic regions, and severely limits the sensitivity of the methodology as the source organism of the short sequences is hard to determine both by mapping and \textit{de novo} assembly methods. Additionally, turnaround times range from several hours to a full days, not ideal for a timely diagnosis and reporting, and require batching of samples to be run cost effectively which, depending on the instrument, can rage from a few samples to a few dozens, adding to the turnaround time and lowering the sensitivity of pathogen detection. In chapter \ref{ch:paper1}, the analysis through \ac{SMg} of 10 samples took between 48-54 hours to complete, which is shorter than culture-based methods if one includes typing. Due to the sequencing depth required to capture the lowest abundant microornanisms in a sample, only the instruments with highest throughput are recommended for metagenomic sequencing \footnote{ \url{https://www.illumina.com/systems/sequencing-platforms.html}}. 

Third generation sequencing technologies have been emerging as a viable alternative to second generation methods as longer sequences offer more contextual information and do not have the limitation of bias-inducing pre-sequencing \ac{PCR} library preparation, as each molecule is sequenced directly \citep{loman_twenty_2015, ari_next-generation_2016}. This results in a lower bias, but a much lower throughput of data and much higher baseline sample input requirement, being more susceptible to sequencing errors \citep{gu_clinical_2019}. Long-reads also have the advantage of resolving structural variations and variants in repetitive regions, which are poorly resolved by short-reads and are often excluded in bioinformatics analysis. Despite this, in chapter \ref{ch:paper2} it was shown that even when hybrid assembly is employed, leveraging both second and third generation methodologies are employed, complete genomic sequences, particularly chimeric ones such as plasmids, are still not fully recovered. 


\subsubsection{Limitations of host sequence contamination}

The unbiased nature of \ac{SMg} allows the sequencing of the nucleic acid of all pathogens (including commensal microbes) and the host. Nearly all \ac{DNA} and \ac{RNA} content in most clinical samples is host (human) derived, this unbiased nature of SMg further lowers the sensitivity of potential pathogen detection as the sequencing library comprises both nucleic acids from the patient and pathogens. The sequence coverage of the pathogen depends on the ratio of host/pathogen nucleic acid present in the sample, with most samples being dominated by human host background \citep{gu_clinical_2019}. The presence of an overwhelming amount of host \ac{DNA} or \ac{RNA} is one of the most important problems to be addressed in \ac{SMg}.

As observed in chapter \ref{ch:paper1}, the number of human reads differed between the 10 samples selected form \ac{SMg}, even when using the same extraction kit and all kits including a human DNA depletion step. This highlights that the ratio between host and microbial DNA or other individual sample characteristics will be the major determinants of the proportion of microbial reads.

The depletion steps aim to decrease the relative proportion of human host background sequences through capture probes, lysis and deoxyribonuclease and/or ribonuclease treatment \citep{gu_clinical_2019}. Theoretically the microbial proportion of the sample is protected within viral capsids and microbial cell walls, but alterations to the microbial composition can still occur. Alternatively, the host sequences can be removed after sequencing, as performed in chapter \ref{ch:paper3}. In this chapter the reads of interest were captured through a mapping approach to a large collection of reference genomes but the opposite methodology can be applied, where the contaminant host sequences are removed (in their majority) my mapping to a human reference genome. This method is not as cost efficient as host DNA depletion in the bench, as a greater proportion of background sequences are sequenced, but the community remains, theoretically, unaltered. 

\subsubsection{Limitations of the bioinformatic analysis}

In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic and surveillance test results. Computational pipelines for metagenomic analysis of \ac{SMg} have unique challenges and requirements, from host depletion, taxonomic classification, to metagenomic \textit{de novo} assembly and binning of \ac{MAG}s and strain characterisation. 

For \textit{in silico} host depletion, only the sequences that align to a human reference genome will be removed, not necessarily removing the totality of human sequences and, in the lens of patient identity protection, leaving the most identifiable sequences behind. In the same principle, when performing taxonomic characterisation, only sequences present in the database used for sequence identification will be reported, with rare pathogens or emerging strains of pathogens not being reported. Additionally, reference databases are often biased towards certain organisms, as well as certain pathogens that are important to distinguish are similar genetically, with current methods not having enough resolution to do so \citep{gu_clinical_2019}. Lastly, contamination with flora and/or reagents should be accounted for as it can limit specificity, hence the importance of using and analysing controls alongside samples. 

There is no standard method for interpreting \ac{SMg} results. Chapters \ref{ch:paper1} and \ref{ch:paper4} highlight how different bioinformatics approaches, and different tools for the same approach, can affect the overall interpretation of the results. In particular, in chapter \ref{ch:paper1} substantial differences were noted between the taxonomic classification tools, with the results being highly dependent on the tools, and especially the database that was chosen for the analysis greatly impacts its applicability in a clinical setting. In chapter \ref{ch:paper4} the performance of each assembler varied depending on the species of interest and its abundance in the sample, with less abundant species presenting a significant challenge for all assemblers. No assembler stood out as an undisputed all-purpose choice for \ac{SMg} assembly, highlighting that efforts are still needed to further improve metagenomic assembler performance.

The \textit{de novo} assembly of raw sequence data is key in metagenomic analysis. It allows recovering draft genomes from a pool of mixed raw reads, yielding longer sequences that offer contextual information and provide a more complete picture of the microbial community. In chapter \ref{ch:paper4} we compared XX assembly tools observing that assemblers branded as metagenomic specific did not consistently outperform other genomic assemblers in \ac{SMg} samples. Additionally, the performance of each assembler varied depending on the species of interest and its abundance in the sample, with less abundant species presenting a significant challenge for all assemblers. No assembler stood out as an undisputed all-purpose choice for short-read metagenomic prokaryote genome assembly, highlighting that efforts are still needed to further improve metagenomic assembler performance.

Furthermore, the constant changes in versions and/or the discontinuation of a bioinformatics tool complicates the standardisation of data analysis. In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic test results. 

\subsection{Better standards in metagenomics for clinical microbiology}

\subsubsection{The use of mock communities}

In chapter \ref{ch:paper4}, its shown that suitable mock communities, reproducing the users’ samples of interest, can be used as a gold standard to evaluate tool performance. 

\subsubsection{Ensuring reproducibility}

\subsubsection{The need for proper benchmarking}

\subsubsection{The adoption of standard specifications}

\subsubsection{The need for intuitive and responsive reports}

\subsection{Crowdsourcing for better standards in metagenomics}