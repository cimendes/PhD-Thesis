\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

The rise of life lost due to microbial pathogens, particularly when associated with the surge of \ac{AMR}, poses a major threat to human health around the world. Optimising the diagnostic process is crucial in managing infectious diseases \citep{vos_global_2020}. Currently, the golden standard for clinical microbiology are culture, antimicrobial susceptibility testing, \ac{PCR}, including syndromic multiplex testing, and serology. Sequencing, when applied, is usually limited to 16S for prokaryotic pathogen identification \citep{greninger_challenge_2018}. 

In this thesis, we have evaluated the use of bioinformatics methods for the analysis of metagenomic data to allow the rapid identification, virulence analysis and antimicrobial susceptibility prediction of pathogens with clinical relevance, from both diagnostic and a surveillance settings. With the widespread use and continuous development of sequencing technologies, bioinformatics has become a cornerstone in modern clinical microbiology. As mentioned previously, the lack of golden standards severely hinders the applicability of bioinformatic methods, particularly in \ac{SMg} \citep{carrico_primer_2018, couto_critical_2018, angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017}. 

Metagenomics, and in particular \ac{SMg}, has emerged as a promising approach for diagnosis from clinical samples and surveillance of organisms of interest from the environment \citep{loman_culture-independent_2013, rossen__2018, schuele_future_2021, chiu_clinical_2019}. A single metagenomics analysis has the potential to detect common, rare and novel pathogens, and provides a broad overall picture of the microbial content present in a sample. Despite this, is often unclear whether a given detected microorganism is a contaminant, coloniser or \textit{bona fide} pathogen, and the lack of golden standards remains one of the biggest challenges when applying these methods in clinical microbiology for diagnosis. 

Several limitations have been identified that, in its current form, curb the applicability of these methods in both clinical and public health microbiology. 

\subsection{Limitation to the application of metagenomics in clinical microbiology}

\subsubsection{Limitations of sequencing technologies}

While the application of genomics in clinical microbiology has been increasing, the translation of genetic information remains challenging. Recent advances in DNA sequencing technologies have expanded their application as a diagnostic tool, but limitations still prevail. After over a quarter of a century of development and maturation, several technologies are available to be used both in research and in the clinic, from the first generation DNA chain termination sequencing to third generation long-read sequencing. Despite this, the main benefit if current clinical microbiology testing paradigm in comparison with genomic approaches is that it allows for cost-effective negative results \citep{greninger_challenge_2018}. The initial capital cost of setting up a genomics capable facility for the use of metagenomics are considerable, having been estimated at around one million United States dollars \citep{greninger_challenge_2018}. Additionally, several factors can reduce the sensitivity and specificity of these methods. Sequencing only one type of molecule, either \ac{DNA} or \ac{RNA}, can lead to the missing of sample components such as RNA viruses or non-replicating DNA viruses, although sequencing all nucleic acids present increases the overall cost per sample \citep{schuele_future_2021}. Additionally to the sample composition, the specimen volume, collection method, transport and sequencing method can affect \ac{SMg} sensitivity, with multiple possible approaches to the wet lab work which require optimisation \citep{petersen_third-generation_2019}. Regarding the sequencing technologies available, each have their own pitfalls. 

First generation sequencing technologies requires the input \ac{DNA} to consist of a pure population of sequences, as each molecule will contribute to the final eletropherogram, as it is a superposition of all of the input molecules \citep{hagemann_overview_2015}. As such, it cannot be applied to metagenomic methodologies.

Second generation sequencing so far represent the most popular technology applied in metagenomics \citep{rossen_practical_2018, loman_twenty_2015, loman_high-throughput_2012, schuele_future_2021}. Second-generation methods require library preparation and an enrichment or amplification step \citep{hagemann_overview_2015}, a time-consuming, bias inducing procedure that is propagated to the resulting data. Another limitation is the size of the outputted sequences that, despite the massive throughput of some of the machines available, requiring a very small \ac{DNA} input load to produce up to billions of sequences \citep{loman_twenty_2015}, ranges from from 45 to 300 bases in length \citep{loman_performance_2012, ari_next-generation_2016}. This is simply not enough to not only transverse the most repetitive genomic regions, and severely limits the sensitivity of the methodology as the source organism of the short sequences is hard to determine both by mapping and \textit{de novo} assembly methods. Additionally, turnaround times range from several hours to a full days, not ideal for a timely diagnosis and reporting, and require batching of samples to be run cost effectively which, depending on the instrument, can rage from a few samples to a few dozens, adding to the turnaround time and lowering the sensitivity of pathogen detection \citep{greninger_challenge_2018, schuele_future_2021}. In chapter \ref{ch:paper1}, the analysis through \ac{SMg} of 10 samples took between 48-54 hours to complete, which is shorter than culture-based methods if one includes typing. Due to the sequencing depth required to capture the lowest abundant microorganisms in a sample, only the instruments with highest throughput are recommended for metagenomic sequencing \footnote{ \url{https://www.illumina.com/systems/sequencing-platforms.html}}. This, however, is not cost-effective in routine diagnostics where samples need to be immediately processed \citep{rossen__2018}. Additionally, as discussed in chapter \ref{ch:paper1}, when applying sample batching, second generation sequencing related phenomena, such as index hopping (also named index switching) or crosstalk (also called sample bleeding) ca introduce false-positive results.

Third generation sequencing technologies have been emerging as a viable alternative to second generation methods as longer sequences offer more contextual information and do not have the limitation of bias-inducing pre-sequencing \ac{PCR} library preparation, as each molecule is sequenced directly \citep{loman_twenty_2015, ari_next-generation_2016}. This results in a lower bias, but a much lower throughput of data and much higher baseline sample input requirement \citep{gu_clinical_2019}. Long-reads also have the advantage of resolving structural variations and variants in repetitive regions, which are poorly resolved by short-reads and are often excluded in bioinformatics analysis. In particular, \ac{ONT} sequencing has emerged as an attractive platform for clinical laboratories to adopt due to its low cost and rapid turnaround time, being able to provide results in almost real time \citep{petersen_third-generation_2019}. Despite this, this method still faces problems in base-calling accuracy when compared with other platforms \citep{gu_clinical_2019}. 

In chapter \ref{ch:paper2} it was shown that even when hybrid assembly with Illumina and \ac{ONT} data is employed, leveraging both second and third generation methodologies, complete genomic sequences, particularly chimeric ones such as plasmids, are still not fully recovered. Although Nanopore sequencing, employed in chapter \ref{ch:paper2} is able to produce reads of up to 2 megabases in length, the biggest drawbacks to date have been a lower throughput of sequence data and a high error rate (approximately 10\%) \citep{petersen_third-generation_2019}. Regardless, rapid advancements are being achieved with new, more accurate flowcells \citep{sereika_oxford_2021}. 

\subsubsection{Limitations of host sequence contamination}

The unbiased nature of \ac{SMg} allows the sequencing of the nucleic acid of all pathogens (including commensal microbes) and the host. Nearly all \ac{DNA} and \ac{RNA} content in most clinical samples is host (human) derived, this unbiased nature of SMg further lowers the sensitivity of potential pathogen detection as the sequencing library comprises both nucleic acids from the patient and pathogens. The sequence coverage of the pathogen depends on the ratio of host/pathogen nucleic acid present in the sample, with most samples being dominated by human host background \citep{gu_clinical_2019}. The presence of an overwhelming amount of host \ac{DNA} or \ac{RNA} is one of the most important problems to be addressed in \ac{SMg}.

As observed in chapter \ref{ch:paper1}, the number of human reads differed between the 10 samples selected form \ac{SMg}, even when using the same extraction kit and all kits including a human DNA depletion step. This highlights that the ratio between host and microbial DNA or other individual sample characteristics will be the major determinants of the proportion of microbial reads.

The depletion steps aim to decrease the relative proportion of human host background sequences through capture probes, lysis and deoxyribonuclease and/or ribonuclease treatment \citep{gu_clinical_2019}. Theoretically the microbial proportion of the sample is protected within viral capsids and microbial cell walls, but alterations to the microbial composition can still occur. Alternatively, the host sequences can be removed after sequencing, as performed in chapter \ref{ch:paper3}. In this chapter the reads of interest were captured through a mapping approach to a large collection of reference genomes but the opposite methodology can be applied, where the contaminant host sequences are removed (in their majority) my mapping to a human reference genome. This method is not as cost efficient as host DNA depletion in the bench, as a greater proportion of background sequences are sequenced, but the community remains, theoretically, unaltered. 

\subsubsection{Limitations of the bioinformatic analysis}

In addition to the variability of wet lab protocols, the bioinformatics for data handling and interpretation can be resource intense. Bioinformatic analysis requires highly trained staff, valid analysis tools, including the reference database, the computational infrastructure, and the creation of standardised procedures \citep{petersen_third-generation_2019}. In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic and surveillance test results. Computational pipelines for metagenomic analysis of \ac{SMg} have unique challenges and requirements, from host depletion, taxonomic classification, to metagenomic \textit{de novo} assembly and binning of \ac{MAG}s and strain characterisation. Additionally, as observed in chapter \ref{ch:paper1} and \ref{ch:paper2}, each sequencing platform, from second (Illumina) and third generation (\ac{ONT}), requires their own data processing steps and quality control metrics. For bioinformatics, the tools that have been developed in the research community for short-read data are not feasible for long-read data. Data interpretation adds an additional level of complexity.  

For \textit{in silico} host depletion, only the sequences that align to a human reference genome will be removed, not necessarily removing the totality of human sequences and, in the lens of patient identity protection, leaving the most identifiable sequences behind. In the same principle, when performing taxonomic characterisation, only sequences present in the database used for sequence identification will be reported, with rare pathogens or emerging strains of pathogens not being reported. In chapter \ref{ch:paper1}, different taxonomic classification tools produced different results for the same sample. Additionally, reference databases are often biased towards certain organisms, as well as certain pathogens that are important to distinguish are similar genetically, with current methods not having enough resolution to do so \citep{gu_clinical_2019}. Lastly, contamination with flora and/or reagents should be accounted for as it can limit specificity, hence the importance of using and analysing controls alongside samples. 

The quality control of the \ac{SMg} reads is in all similar to any genomics workflow,  an essential prerequisite that involves quality trimming and sequencing adaptor removal. In opposition, the assembly step, where reads are stitched into longer fragments, referred to as contigs, is usually tailored to \ac{SMg} analysis. These contigs are longer sequences that offer better contextual information than reads alone and provide a more complete picture of the microbial community than the species composition, and is usually followed by reconstructing the individual genes and species. In chapter \ref{ch:paper4} both a metagenomic and a traditional genomic employed in a two pronged approach to guarantee the highest change of success when assembling full \ac{DENV} genomes from the metagenomic samples. As discussed in chapter \ref{ch:paper5}, several dedicated metagenomic assembly tools for short-read data are available, generally assumed to perform better when dealing with the complex \ac{SMg} samples. Despite this assumption, assemblers branded as metagenomic specific did not consistently outperform other genomic assemblers in the metagenomic samples analysed. Additionally, the performance of each assembler varied depending on the species of interest and its abundance in the sample, with less abundant species presenting a significant challenge for all assemblers.

Other areas of bioinformatics not directly covered in this thesis, such as contig binning, also provide their own set of challenges. In order to reconstruct genomes using heterogeneous sequencing data, contig grouping based on an individual genome of origin or metagenomics binning is done, either by supervised methods, relying on taxonomic information in a database, or unsupervised clustering. The first, like taxonomic assignment, is limited  to what is available in the database used, whereas the latter, despite not requiring a priori knowledge, tend to be very computational expensive with very variable accuracy   \citep{bharti_current_2021}. 

There is no standard method for interpreting \ac{SMg} results. Chapters \ref{ch:paper1} and \ref{ch:paper4} highlight how different bioinformatics approaches, and different tools for the same approach, can affect the overall interpretation of the results. In particular, in chapter \ref{ch:paper1} substantial differences were noted between the taxonomic classification tools, with the results being highly dependent on the tools, and especially the database that was chosen for the analysis greatly impacts its applicability in a clinical setting. 

\subsection{Better standards in metagenomics for clinical microbiology}

\subsubsection{The need for proper benchmarking}

The constant changes in versions and/or the discontinuation of a bioinformatics tool complicates the standardisation of data analysis. In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic test results. With the lack of proper benchmarks to validate in what approach is to be followed, no hope of standardisation can be achieved. 

In chapter \ref{ch:paper5}, 12 \textit{de novo} assemblers were benchmarked with the same metagenomic sample, selected based on the date of last update. No assembler stood out as an undisputed all-purpose choice for short-read metagenomic prokaryote genome assembly, highlighting that efforts are still needed to further improve metagenomic assembler performance. This arose directly from the need to pick an assembler to implement in chapter \ref{ch:paper4}, where ultimately a two-pronged approach was followed including both a metagenomic and a traditional genomic assembler. 

Despite these efforts, this is just a small subset of the commonly bioinformatics methods applied to \ac{SMg} have been assessed. Regarding the binning process, disparate results have been observed \citep{bharti_current_2021, yue_evaluating_2020, yang_review_2021, sczyrba_critical_2017}, highlighting that the need for proper benchmark cannot be limited to the tools itself, but also the standardisation of the data used for the benchmark. Similarly, the same has been observed for taxonomic assignment \citep{sczyrba_critical_2017,ye_benchmarking_2019, tamames_assessing_2019}.

\subsubsection{The use of mock communities}

In chapter \ref{ch:paper5}, its shown that suitable mock communities, reproducing the usersâ€™ samples of interest, can be used as a gold standard to evaluate tool performance. Several well characterised mock communities are currently available to be used by the community when assessing and benchmarking software suitable for \ac{SMg} analysis. Chapter \ref{ch:paper5} features the ZymoBIOMICS Microbial Community Standard, composed of eight bacterial species and two fungi, and is available commercially, with reference sequences made publicly available, with both even and logarithmically distribution of species\footnote{\url{https://www.zymoresearch.com/collections/zymobiomics-microbial-community-standards}}. Still, ZymoBIOMICS Microbial Community Standards might not be representative of the metagenomic complexity of the samples of interest of most researchers, its relative simplicity means that the results shown probably represent a best-case scenario. 

Alternatively, more complex communities, with good metadata and reference genomes publicly available, are available for acquisition and sequencing. \ac{ATCC} 10 Strain Even Mix Genomic Material
MSA-1000\footnote{\url{https://www.atcc.org/products/msa-1000}} and 20 Strain Even Mix Genomic Material MSA-1002\footnote{\url{https://www.atcc.org/products/msa-1002}} consists of 10 and 20, respectively, fully sequenced, characterised, and authenticated \ac{ATCC} Genuine Cultures mixed evenly, selected based on pathogen relevance. It also provides Gut Microbiome Genomic Mix MSA-1006\footnote{\url{https://www.atcc.org/products/msa-1006}} composed of an even mixture of genomic DNA prepared from 12 fully sequenced, characterised, and authenticated bacterial species observed in normal and atypical gut microbial communities. 

For the replication of more complex communities, the MICROBIOME Community of Special Interest\footnote{\url{https://www.microbiome-cosi.org/}} makes available through their \ac{CAMI} initiative\footnote{\url{https://data.cami-challenge.org/}}, makes available several datasets, varying in complexity, replicating several communities such as human microbiome and the rhizosphere. Sources of truth are not made available, including the complete genomes of the community profile, so comparison are difficult. 

\subsubsection{Ensuring reproducibility}

One of the biggest challenges when dealing with metagenomic data is the lack of golden standards, although major efforts are being made on the standardisation and assessment of software, both commercial and open source \citep{angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017, couto_critical_2018}. A plethora of open source tools are available specifically for metagenomic data, both short and long read data, and several combinations of these tools can be used to characterise the causative agent in a patient's infection in a fraction of time required by the traditional methods. Alongside, there are several commercial alternatives, such as CLC Genomics Workbench (QIAGEN Bioinformatics), Taxonomer \citep{flygare_taxonomer_2016} and BaseSpace (Illumina), that offer ready to use complete workflows at the cost of lack of transparency, reproducibility and control in the analysis.

There are several steps that can be implemented to ensure the transparency and reproducibility of the chosen workflow. Favouring open-source tools, with clear documentation describing the methodology implemented, and stating the version of the software used and which parameters were used enables the comparison of results. This can be simplified by containerising all the software tools with one of the many solutions available, like Docker\footnote{\url{https://www.docker.com/}} or Singularity \citep{kurtzer_singularity_2017}. The use of workflow managers, like nextflow \citep{di_tommaso_nextflow_2017} or the Galaxy Project \citep{afgan_galaxy_2018}, will push reproducibility to the next level by taking advantage of the containerisation and scalability, enabling the workflow to be executed with the exact same parameters in the same conditions in a multitude of different environments. 

In chapters \ref{ch:paper4} and \ref{ch:paper5}, one of the main key objectives was to  

\subsubsection{The need for intuitive and responsive reports}

\subsubsection{The adoption of standard specifications}

\subsubsection{Crowdsourcing for better standards}