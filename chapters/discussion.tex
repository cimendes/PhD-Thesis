\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

The rise of life lost due to microbial pathogens, particularly when associated with the surge of \ac{AMR}, poses a significant threat to human health around the world. Optimising the diagnostic process is crucial in managing infectious diseases \citep{vos_global_2020}. Currently, the golden standard for clinical microbiology are culture, antimicrobial susceptibility testing, \ac{PCR}, including syndromic multiplex testing, and serology. Sequencing, when applied, is usually limited to 16S for prokaryotic pathogen identification \citep{greninger_challenge_2018}. 

In this thesis, we have evaluated the use of bioinformatics methods for the analysis of metagenomic data to allow the rapid identification, virulence analysis and antimicrobial susceptibility prediction of pathogens with clinical relevance, from both diagnostic and surveillance settings. With the widespread use and continuous development of sequencing technologies, bioinformatics has become a cornerstone in modern clinical microbiology. As mentioned previously, the lack of golden standards severely hinders the applicability of bioinformatic methods \citep{carrico_primer_2018, couto_critical_2018, angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017}. 

Metagenomics, and in particular \ac{SMg}, has emerged as a promising approach for diagnosis from clinical samples and surveillance of organisms of interest from the environment \citep{loman_culture-independent_2013, rossen__2018, schuele_future_2021, chiu_clinical_2019}. A single metagenomics analysis has the potential to detect common, rare and novel pathogens, and provides a broad overall picture of the microbial content present in a sample. Despite this, it is often unclear whether a given detected microorganism is a contaminant, coloniser or \textit{bona fide} pathogen. 

Several limitations have been identified that, in their current form, curb the applicability of these methods in both clinical and public health microbiology. 

\section{Limitation to the application of metagenomics in clinical microbiology}

\subsection{Limitations of sequencing technologies}

While the application of genomics in clinical microbiology has been increasing, the translation of genetic information remains challenging. Recent advances in DNA sequencing have expanded their application as a diagnostic tool, but limitations still prevail. After over a quarter of a century of development and maturation, several technologies are available to be used both in research and in the clinic, from the first generation DNA chain termination sequencing to third generation long-read sequencing. Despite this, the main benefit of the current clinical microbiology testing paradigm, in comparison with genomic approaches, is that it allows for cost-effective negative results \citep{greninger_challenge_2018}. The initial capital cost of setting up a genomics-capable facility for the use of metagenomics is considerable \citep{greninger_challenge_2018}. Several factors can reduce the sensitivity and specificity of these methods. Sequencing only one type of molecule, either \ac{DNA} or \ac{RNA}, can lead to the missing of sample components such as RNA viruses or non-replicating DNA viruses, although sequencing all nucleic acids present increases the overall cost per sample \citep{schuele_future_2021}. Additionally to the sample composition, the specimen volume, collection method, transport and sequencing method can affect \ac{SMg} sensitivity, with multiple possible approaches to the wet lab work which require optimisation \citep{petersen_third-generation_2019}. Regarding the sequencing technologies available, each has its own pitfalls. 

First generation sequencing technologies require the input \ac{DNA} to consist of a pure population of sequences, as each molecule will contribute to the final electropherogram, as it is a superposition of all of the input molecules \citep{hagemann_overview_2015}. As such, it cannot be applied to metagenomic methodologies.

Second generation sequencing so far represent the most popular technology applied in metagenomics \citep{rossen_practical_2018, loman_twenty_2015, loman_high-throughput_2012, schuele_future_2021}. Second-generation methods require library preparation including an enrichment or amplification step \citep{hagemann_overview_2015}, a time-consuming, bias-inducing procedure that is propagated to the resulting data. Another limitation is the size of the outputted sequences that, despite the massive throughput of some of the machines available, requiring a very small \ac{DNA} input load to produce up to billions of sequences \citep{loman_twenty_2015}, ranges from 45 to 300 bases in length \citep{loman_performance_2012, ari_next-generation_2016}. This is simply not enough to transverse the most repetitive genomic regions, and severely limits the sensitivity of the methodology as the source organism of the short sequences is hard to determine both by mapping and \textit{de novo} assembly methods. Furthermore, turnaround times range from several hours to full days, not ideal for a timely diagnosis and reporting, and require batching of samples to be run cost-effectively which, depending on the instrument, can range from a few samples to a few dozens, adding to the turnaround time and lowering the sensitivity of pathogen detection \citep{greninger_challenge_2018, schuele_future_2021}. In Chapter \ref{ch:paper1}, the analysis through \ac{SMg} of 10 samples took between 48-54 hours to complete, which is shorter than culture-based methods if one includes typing. Due to the sequencing depth required to capture the lowest abundant microorganisms in a sample, only the instruments with the highest throughput are recommended for metagenomic sequencing \footnote{ \url{https://www.illumina.com/systems/sequencing-platforms.html}}. This, however, is not cost-effective in routine diagnostics where samples need to be immediately processed \citep{rossen__2018}. Also, as discussed in Chapter \ref{ch:paper1}, when applying sample batching, second generation sequencing-related phenomena such as index hopping (also named index switching) or crosstalk (also called sample bleeding) can introduce false-positive results.

Third generation sequencing technologies have emerged as a viable alternative to second generation methods as longer sequences offer more contextual information and do not have the limitation of bias-inducing pre-sequencing \ac{PCR} library preparation, as each molecule is sequenced directly \citep{loman_twenty_2015, ari_next-generation_2016}. This results in a lower bias, but a much lower throughput of data and a much higher baseline sample input requirement \citep{gu_clinical_2019}. Long-reads also have the advantage of resolving structural variations and variants in repetitive regions, which are poorly resolved by short-reads and are often excluded in bioinformatics analysis. In particular, \ac{ONT} sequencing has emerged as an attractive platform for clinical laboratories to adopt due to its low cost and rapid turnaround time, being able to provide results in almost real-time \citep{petersen_third-generation_2019}. Despite this, this method still faces problems in base-calling accuracy when compared with other platforms \citep{gu_clinical_2019}. In Chapter \ref{ch:paper2} it was shown that even when hybrid assembly with both Illumina and \ac{ONT} data is employed, leveraging second and third generation methodologies, complete genomic sequences, particularly chimeric ones such as plasmids, are still not fully recovered. Although Nanopore sequencing is able to produce reads of up to 2 megabases in length, the biggest drawbacks to date have been a lower throughput of sequence data and a high error rate (approximately 10\%) \citep{petersen_third-generation_2019}. Regardless, rapid advancements are being achieved with new, more accurate flowcells and better base calling software \citep{sereika_oxford_2021}. 

\subsection{Limitations of host sequence depletion}

The unbiased nature of \ac{SMg} allows the sequencing of the nucleic acid of all microbes (including commensal) and the host. Nearly all \ac{DNA} and \ac{RNA} content in most clinical samples is host (human) derived, further lowering the sensitivity of potential pathogen detection \citep{gu_clinical_2019}. Therefore, the presence of an overwhelming amount of host \ac{DNA} or \ac{RNA} is one of the most important problems to be addressed in \ac{SMg}.
As observed in Chapter \ref{ch:paper1}, the number of human reads differed between the 10 samples selected form \ac{SMg}, even when using the same extraction kit, and all kits including a human DNA depletion step. This highlights that the ratio between host and microbial DNA is a major determinant of the proportion, and therefore the detection of microbial reads.

There are depletion steps that aim to decrease the relative proportion of human host background sequences through capture probes, lysis and deoxyribonuclease and/or ribonuclease treatment \citep{gu_clinical_2019}. Theoretically, the microbial proportion of the sample is protected within viral capsids and microbial cell walls, but alterations to the microbial composition can still occur. Alternatively, the host sequences can be removed after sequencing, as performed in Chapter \ref{ch:paper3}. In this Chapter, the reads of interest were captured through a mapping approach to a large collection of reference genomes but the opposite methodology can be applied, where the contaminant host sequences are removed (in their majority) by mapping to a human reference genome. This method is not as cost-efficient as host DNA depletion in the bench, as a greater proportion of background sequences are sequenced, but the community composition remains, theoretically, unaltered. 

\subsection{Limitations of the bioinformatic analysis}

In addition to the variability of wet lab protocols, the bioinformatics for data handling and interpretation can be resource intensive. Bioinformatic analysis requires highly trained staff, valid analysis tools, including the reference database, good computational infrastructure, and the creation of standardised procedures \citep{petersen_third-generation_2019}. In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic and surveillance test results. Computational pipelines for the analysis of \ac{SMg} have unique challenges and requirements, from host depletion and taxonomic classification, to metagenomic \textit{de novo} assembly, binning of \ac{MAG}s and strain characterisation. Additionally, as observed in Chapters \ref{ch:paper1} and \ref{ch:paper2}, each sequencing platform, from second (Illumina) and third generation (\ac{ONT}), requires their own data processing steps and quality control metrics as the tools that have been developed in the research community for short-read data are not feasible for long-read data. Data interpretation adds an additional level of complexity.  

For \textit{in silico} host depletion, only the sequences that align to a human reference genome will be removed, not necessarily removing the totality of human sequences and, in the lens of patient identity protection, leaving the most identifiable sequences behind. In the same principle, when performing taxonomic characterisation, only sequences present in the database used for sequence identification will be identified, with rare pathogens or emerging strains of pathogens not being reported. In Chapter \ref{ch:paper1}, different taxonomic classification tools produced different results for the same sample. Additionally, reference databases are often biased towards certain organisms, as well as certain pathogens that are important to distinguish are similar genetically, with current methods not having enough resolution to do so \citep{gu_clinical_2019}. Lastly, contamination with flora and/or reagents should be accounted for as it can limit specificity, hence the importance of using and analysing controls alongside samples. 

The quality control of the \ac{SMg} reads is in all similar to any genomics workflow, an essential prerequisite that involves quality trimming and sequencing adaptor removal. In opposition, the assembly step, where reads are stitched into longer fragments, referred to as contigs, is usually tailored to \ac{SMg} analysis. These contigs are longer sequences that offer better contextual information than reads alone and provide a more complete picture of the microbial community than just the species composition and are usually followed by reconstructing the individual genes and species. In Chapter \ref{ch:paper4} both metagenomic and traditional genomic assemblers are employed in a two-pronged approach to guarantee the highest chance of success when recovering full \ac{DENV} genomes from the metagenomic samples. As discussed in Chapter \ref{ch:paper5}, several dedicated metagenomic assembly tools for short-read data are available, generally assumed to perform better when dealing with the complex \ac{SMg} samples. Despite this assumption, assemblers branded as metagenomic specific did not consistently outperform other genomic assemblers in the metagenomic samples analysed. Additionally, the performance of each assembler varied depending on the species of interest and its abundance in the sample, with less abundant species presenting a significant challenge for all assemblers.

Other areas of bioinformatics not directly covered in this thesis, such as contig binning, also provide their own set of challenges. In order to reconstruct genomes using heterogeneous sequencing data, contig grouping based on an individual genome of origin is done, either by supervised methods, relying on taxonomic information in a database, or unsupervised clustering. The first, like taxonomic assignment, is limited  to what is available in the database used, whereas the latter, despite not requiring \textit{a priori} knowledge, tends to be very computationally expensive and with very variable accuracy \citep{bharti_current_2021}. 

There is no standard method for interpreting \ac{SMg} results. Chapters \ref{ch:paper1} and \ref{ch:paper4} highlight how different bioinformatics approaches, and different tools for the same approach, can affect the overall interpretation of the results. In particular, substantial differences were noted between the taxonomic classification tools in Chapter \ref{ch:paper1}, with the results being highly dependent on the tools, and especially the database that was chosen for the analysis greatly impacts its applicability in a clinical setting. 

\section{Better standards in metagenomics for clinical microbiology}

\subsection{The need for better assessment}

The constant changes in versions and/or the discontinuation of a bioinformatics tool complicates the standardisation of data analysis. In routine settings, automation and standardisation of the analysis are significant for the reliability of the diagnostic test results. With the lack of proper benchmarks to validate what approach is to be followed, no hope of standardisation can be achieved.


\subsubsection{Performing proper benchmarking of software}

In Chapter \ref{ch:paper1}, the suitability of \ac{SMg} for the microbiological diagnosis was assessed, with particular emphasis on the bioinformatic process. In total, 3 different bioinformatics pipelines were evaluated to identify those which could provide a clinical microbiologist with the maximum of relevant and accurate information: an open-source Unix-based approach, and two commercial alternatives, Basespace and CLC Genomics Workbench. No approach outperformed the other, and very disparate results were obtained for each one. One of the limitations of this study was that, although several approaches were used, no comprehensive assessment of the tools available for each step of the workflow was performed (quality control, removal of host sequences, taxonomic identification, gene detection and \textit{de novo} assembly). Currently, no open-source, user-friendly, one-stop, comprehensive metagenomics toolkit with a visual user interface for shotgun metagenomics analysis is available, but efforts are being made to change this paradigm. Alternatives, such as Anvi'o \citep{eren_anvio_2015} for shotgun metagenomics, and Qiime2 \citep{bolyen_reproducible_2019} for metataxonomics, leverage the individual components in a streamlined manner, but require command-line familiarity. Still, each individual component of such software should be individually evaluated.

In Chapter \ref{ch:paper4}, where ultimately a two-pronged approach was followed including both a genomic and a metagenomic \textit{de novo} assembler. This arose from the need to recover complete CDS sequences, where none of the alternatives on their own was able to perform consistently. Despite several benchmarking studies being available \citep{miller_assembly_2010, earl_assemblathon_2011, bradnam_assemblathon_2013, liao_current_2019, chen_benchmarking_2020, sczyrba_critical_2017, meyer_critical_2022}, the assemblers benchmarked and the datasets used are not consistent, usually limited to a couple of different tools that do not reflect the current landscape of possible \textit{de novo} assembly software. These benchmarks are also usually coupled with the release of a new tool, therefore often skewed towards certain results and do not offer an unbiased assessment. 

In Chapter \ref{ch:paper5}, 12 \textit{de novo} assemblers were benchmarked with the same metagenomic samples, selected based on the date of the last update to the software and with a cut-off in 2015. No assembler stood out as an undisputed all-purpose choice for short-read metagenomic prokaryote \textit{de novo} genome assembly, highlighting that efforts are still needed to further improve metagenomic assembler performance. 

Despite these efforts, just a small subset of the common bioinformatics methods applied to \ac{SMg} have been assessed. Regarding the binning process, disparate results have been observed \citep{bharti_current_2021, yue_evaluating_2020, yang_review_2021, sczyrba_critical_2017}, highlighting that the need for proper benchmark cannot be limited to the tools itself, but also the standardisation of the data used for the benchmark. Similarly, the same has been observed for taxonomic assignment \citep{sczyrba_critical_2017,ye_benchmarking_2019, tamames_assessing_2019}.

\subsubsection{The use of mock communities}

Chapter \ref{ch:paper5} shows that suitable mock communities, reproducing the users’ samples of interest, can be used as a gold standard to evaluate tool performance. Several well-characterised mock communities are currently available to be used by the community when assessing and benchmarking software suitable for \ac{SMg} analysis. Chapter \ref{ch:paper5} features the ZymoBIOMICS Microbial Community Standard, composed of eight bacterial species and two fungi, and is available commercially, with reference sequences made publicly available, with both even and logarithmically distribution of species\footnote{\url{https://www.zymoresearch.com/collections/zymobiomics-microbial-community-standards}}. Still, ZymoBIOMICS Microbial Community Standards might not be representative of the metagenomic complexity of the samples of interest of most researchers, its relative simplicity means that the results shown probably represent a best-case scenario. Furthermore, in the logarithmically distributed sample, six replicons have a coverage lower than one time, rendering it unrecoverable through assembly. To address this, two other community standards were analysed: the BMock12 \citep{sevim_shotgun_2019} and the Gut-Mix-RR \citep{amos_developing_2020}. 

The BMock12 community is composed of twelve bacterial species, spanning eight genera. This community dataset includes several closely related strains: two replicons of \textit{Halomonas} sp., three replicons of the \textit{Micromonospora} and two replicons of \textit{Marinobacter} sp. The challenges of this data set are to assemble the genomes of the two Marinobacter and the two Halomonas strains, which come at 85\% and 99\% ANIb, respectively. Samples were made available of the sequencing products of this community both in second (Illumina) and third-generation sequencing (PacBio and Oxford Nanopore), expanding its utility for the benchmarking of both short-read, long-read and hybrid software. With worth noting that the molarity of each genome in the mock community is not even throughout, resulting in the extreme overrepresentation of some taxa in relation to others. This can have the benefit of providing a form of a limit of detection assessment, depending on the type of analysis performed. Still, besides the challenges that this dataset present, is still a rather simplistic picture of a metagenomic sample.

The Gut-Mix-RR community dataset, and its uneven distribution of species counterpart Gut-HiLo-RR, spans five phyla, thirteen families, sixteen genera and nineteen species, comprising twenty strains that aim to emulate the healthy human gut microbiota. The strains were obtained from Leibniz Institute DSMZ-German Collection of Microorganisms and Cell Cultures GmbH. The genomes in the mock community are fairly diverse, with the challenging genomes being the two \textit{Bifidobacterium longum} subspecies (ANIb=0.95). It’s worth denoting that no complete genome is available for eight of the genomes, including one of the \textit{Bifidobacterium longum} subspecies. This makes the assessment of results challenging, if not impossible, as no source of absolute truth is available. 

For the replication of more complex communities, the MICROBIOME Community of Special Interest\footnote{\url{https://www.microbiome-cosi.org/}} makes available, through their \ac{CAMI} initiative\footnote{\url{https://data.cami-challenge.org/}}, several datasets varying in complexity, replicating several communities such as human microbiome and the rhizosphere \citep{sczyrba_critical_2017, meyer_critical_2022}. Unfortunately, sources of truth are not made available, including the complete genomes of the community profile, and assessment of results is difficult, if not impossible. Despite their popularity, no information is provided in the manuscripts or in the supplemental materials about the source of the genomes in each dataset. It is only mentioned that the community composition was designed according to specified criteria. Additionally, only assemblies are made available, and not the source references for the generation of the mock datasets. These assemblies are, unfortunately, often the result of a pooled assembly and are extremely fragmented, with N50 ranging from 18406 basepairs in 56168 contigs to 58988 basepairs in 508968 contigs. This makes this dataset not suitable for benchmark studies, which require defined mock communities as a source of truth. 

The Fishing in the Soup dataset \citep{grutzke_fishing_2019} is a mock community containing DNA of foodborne bacteria. The standard consists of 34 equimolarly pooled bacterial DNAs belonging to 17 genera and 30 species, elected based on their incidence in foodstuff and pathogenicity. Despite its diversity, not all reference sequences are available, and the used reference sequences are not the ones corresponding to the sequenced strains in some cases. Although two sets of the same species replicons are present (\textit{Bacillus cereus}, \textit{Salmonella enterica}, \textit{Clostridium perfringens}), their ANIb is lower than the BMock12 \textit{halomonas} replicons, and no independent references for different strains of the same species are provided. 

Similarly to the ZymoBIOMICS Microbial Community Standard, communities with good metadata and reference genomes publicly available are available for acquisition and sequencing from \ac{ATCC}. The \ac{ATCC} 10 Strain Even Mix Genomic
Material MSA-1000\footnote{\url{https://www.atcc.org/products/msa-1000}} and 20 Strain Even Mix Genomic Material MSA-1002\footnote{\url{https://www.atcc.org/products/msa-1002}} consists of, respectively, 10 and 20, fully sequenced, characterised, and authenticated \ac{ATCC} Genuine Cultures mixed evenly, selected based on pathogen relevance. It also provides Gut Microbiome Genomic Mix MSA-1006\footnote{\url{https://www.atcc.org/products/msa-1006}} composed of an even mixture of genomic DNA prepared from 12 fully sequenced, characterised, and authenticated bacterial species observed in normal and atypical gut microbial communities. Just like ZymoBIOMICS Microbial Community Standards, the simplicity of the ATCC mock communities might not be representative of the metagenomic complexity of the samples of interest to most researchers, representing simplistic scenarios with limited usability.

No ideal solution is currently available in the research community or the market, highlighting the need for better, more diverse and well-described community standards representing realistic scenarios. 


\subsection{The need for better reproducibility}

The analysis of biological data is driven by the development of a myriad of open-source software tools, each carrying out a specialised step, that then can be chained together to generate new knowledge and insights. However, as with any complex system, susceptibility to variation can cause the entire system to collapse. Such variation can be the use of different operating systems, the availability of computational resources, and ambiguities with tool versioning and documentation \citep{wratten_reproducible_2021}. 

One of the biggest challenges when dealing with metagenomic data is the lack of golden standards, although major efforts are being made on the standardisation and assessment of software, both commercial and open source \citep{angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017, couto_critical_2018}. A plethora of free-to-use, open-source tools are available specifically for metagenomic data, both short and long-read data, and several combinations of these tools can be used to characterise the causative agent in a patient's infection in a fraction of the time required by the traditional methods. Unfortunately, as observed in Chapter \ref{ch:paper5}, massive differences in success and performance were observed in software built for the same purpose. In this case, the \textit{de novo} assembly of metagenomic genomic data. There are several steps that can be implemented to ensure the transparency and reproducibility of the chosen workflow for metagenomic analysis, regardless of the tools chosen. 

In Chapters \ref{ch:paper4} and \ref{ch:paper5}, one of the main key objectives was to a fully reproducible, scalable workflow for one, the analysis and characterization of \ac{DENV} directly from metagenomic samples, and two, evaluate the metagenomic \textit{de novo} assembly process through the benchmark of the most commonly used tools. For that, several steps were implemented.

\subsubsection{The need for easy software distribution}

Favouring open-source tools, with clear documentation describing the methodology implemented, and stating the version of the software used and which parameters were used enables the comparison of results. This requires good management of the software installation, and its dependencies, for this information to be trackable and distributed to the community. Bioinformatics software comes in a variety of programming languages and requires diverse installation methods. This heterogeneity makes the management of software complicated, error-prone, and time-consuming. To reproduce the same software on different machines, full control of the computational environment is required, which usually requires administrative privileges that are often not available.  

To solve this issue several options have risen throughout time. For language-specific package managers, PyPi\footnote{\url{https://pypi.org/}}, the Python's Package Indexer, or CRAN\footnote{\url{https://cran.r-project.org/}}, the Comprehensive R Archive Network, follow the principle that precompiled binary distributions are hosted in a centralised service. This makes the sharing of software easier as a determined version of a package can be downloaded and executed by the user without the need for compilation from source. Still, the installation is system-wide, and conflicts can arise. 

Efforts, such as Conda package manager\footnote{\url{https://conda.io}}, and in particular Bioconda \citep{gruning_bioconda_2018}, have become a popular means to overcome these challenges for all major operating systems. Package managers, such as Conda, normalise software installations across ecosystems by describing each software with a human-readable "recipe" that defines its dependencies, as well as a simple "build script" that performs the steps necessary to build and install the software. These installations happen in an isolated environment, obliviating the reliance on system-wide access privileges. This way, the software can exist in an isolated form, allowing the control of software versions and the versions of its dependencies without conflicts arising if a given package gets updated. Additionally, the current status of an environment can be stored in a file and distributed to collaborators. 

Container software, like Docker\footnote{\url{https://www.docker.com/}} or Singularity \citep{kurtzer_singularity_2017}, came to simplify this process by containerising all the software tools and required run environments. These tools are changing the way scientists and researchers develop, deploy and exchange scientific software. Containers constitute lightweight software components and libraries that can be quickly packaged, are designed to run anywhere, and are useful and essential tools to leverage bioinformatics software reproducibility \citep{boettiger_introduction_2015,gruening_recommendations_2019}. Their principle is that a container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. In comparison with alternatives such as traditional Virtual Machines, containers are much more lightweight, running directly in the host operating system, in an isolated manner, while containing all requirements of software needed to run it, such as the code, system tools and system libraries. Their biggest advantage is that a software container will run the same regardless of the environment it is deployed as it runs as an isolated process in user space. Still, one of the main disadvantages is that when a container is created, it's not possible to backtrack its creation, therefore the exact dependencies installed are unknown. It becomes a problem of "it simply works" without having the ability to know exactly how, so although reproducible, containers are not transparent. 

\subsubsection{The need for Workflow managers}

Bioinformatic workflows managers offer integration with containers and package managers such as conda while providing portability, reproducibility, automatic resource management and native cloud support, not being limited to running in a local environment. 
The use of workflow managers, like nextflow \citep{di_tommaso_nextflow_2017}, Snakemake or the Galaxy Project \citep{afgan_galaxy_2018}, pushes reproducibility to the next level, enabling the workflow to be executed with the exact same parameters in the same conditions in a multitude of different environments \citep{wratten_reproducible_2021}. 

Traditional computational pipelines are usually developed using custom scripts, being highly coupled to their local compute infrastructure. Furthermore, they lack the option to resume failed steps, and often lack parameter tracking, tool versioning and require manual installation, making them difficult to share and maintain, greatly hindering reproducibility. Workflow managers, being reactive workflow frameworks, allow the creation of pipelines with asynchronous, and therefore implicitly parallelised data streams. In the case of Nextflow, it provides its own domain language for building a pipeline, which can cause the learning curve too steep, but allows the incorporation of existing tools and scripts written in other languages, offering the possibility to effortlessly port tools over and minimising refactoring. Although the creation of workflow pipelines is designed for bioinformaticians familiar with programming, its execution is for everyone. 

Another advantage of workflow managers is the automation of input parameters and software tool version tracking, often generating reports with detailed information regarding its execution. Re-entrancy allows users to run a pipeline from its last successfully executed step, rather than from the beginning, in the case of disruption. They also process the intermediate files generated during an execution of a pipeline automatically. This is a true paradigm shift where monolithic pipelines are replaced with flexible alternatives, simplifying the implementation of robust and complex analysis while providing additional features that help optimise resource management and reproducibility.  

\subsubsection{The need for Version Control}

Even through the use of container software coupled with workflow managers, these approaches are not, \textit{per se}, versionable. Version control systems, such as Git \citep{chacon2014pro}, were designed not only to solve the problem of multiple people working on the same code (collaboration), but also on how to store versions properly. Through Git, a stage off a project can be saved, alongside a message, which allows backtracking to previous versions, and auditability to track what changes happened when, and by whom. 

Remote repositories, such as GitHub\footnote{\url{https://github.com/}}, the single largest host for Git repositories, are the central point of collaboration for millions of developers and projects. A large percentage of all Git repositories are hosted on GitHub, and many open-source projects use it for Git hosting, issue tracking, code review, and code dissemination. 

\subsubsection{The need for Open Integration Testing }

As explored in chapter \ref{ch:paper7}, the use of containers, workflow managers and proper version control do not guarantee the correct executability of a given pipeline. Testing is a critical aspect of scientific software development, either through the traditional technique approach where the code is tested after its development, or through test driven development approach where a suite of test if first created and the code developed iteratively to solve the tests \citep{krafczyk_scientific_2019}. Despite this, automated software testing remains underused in scientific software, an example of good software engineering principles not yet widely adopted by the community. 

When writing or maintaining software, implementing software tests can greatly help to prevent errors and facilitate code development. In Chapter \ref{ch:paper7}, we defined a set of seven recommendations for researchers in microbial bioinformatics looking to develop software tests. Good software engineering practices are sometimes lacking in the field of microbial bioinformatics as many researchers receive little or no formal training in software engineering. The established set of recommendations aims to aid researchers planning to implement software tests alongside their software development practises. Ultimately, the integration of automatic software testing with version control systems, workflow managers and container software is a good strategy to ensure reproducibility of code and analysis in metagenomics for clinical microbiology. 

\subsection{The need for better Interpretability}

Understanding how to report complex genomic test results to stakeholders who may have varying familiarity with genomics, including clinicians, laboratorians, epidemiologists, and researchers, is critical to the successful and sustainable implementation metagenomics in clinical microbiology. One of the main focuses of chapters \ref{ch:paper3} and \ref{ch:paper4} was the production of interactive, responsive and intuitive reports for the developed workflows. The interactive HTML reports developed provide an intuitive platform for data exploration, allowing the user to highlight specific samples, filter and re-order the data tables, and export the plots as needed. 

In chapter \ref{ch:paper3} the report provided at the end of the workflow execution contains all results divided into four sections: report overview, tables, charts and phylogenetic tree. The report overview and all tables allow for selection, filtering and highlighting of particular samples in the analysis. All tables have information on if a sample failed or passed the quality control metrics highlighted by green, yellow or red signs for pass, warning and fail messages, respectively

In chapter \ref{ch:paper4}, the interactive report provides an intuitive platform for data exploration, allowing the user to easily sift through global and reference specific performance metrics for each sample, as well as providing information on the assemblers executed to allow traceability of the results. Producing an extensive, metric rich report allows users interested in different aspects of assembler performance to make informed decisions, particularly when choosing among the top-performing assemblers, which show only minor differences.

Design Study Methodology, a human centred approach drawn from the information visualisation domain, to redesign an existing clinical report has been proposed previously \citep{crisan_evidence-based_2018}, integrating quantitative and qualitative feedback for representing complex microbial genomic data. We further extend this by providing interactive HTML reports, possible to be shared as a traditional report, but with the advantage of allowing a more in-depth exploration of the results through explorable tables and plots. 

\subsection{The need for better Interoperability}

Despite this richness in genomic information, the same is not observed for the metadata that accompanies it. Chapters \ref{ch:paper5} and \ref{ch:paper6} propose two metadata specification schemas, the first focused on the reporting of \ac{AMR} from genomic data, and the second a minimum metadata standard for the sharing of \ac{SARS-CoV-2} genomic information in public repositories, the latter already adopted by NCBI Submission Portal. 

Similarly to what has been observed in the \ac{SARS-CoV-2} response, where an unprecedented deployment of genomic surveillance worldwide has been observed, with the application of metagenomic to clinical microbiology will require the standardisation of procedures, one of them being the metadata collected and how the results are displayed to stakeholders. 

Contextual data consists of sample metadata (e.g., collection date, sample type, geographical location of sample collection), as well as laboratory (e.g., date and location testing, cycle threshold (CT) values), clinical outcomes (e.g., hospitalisation, death, recovery), epidemiological (e.g., age, gender, exposures, vaccination status) and methods (e.g., sampling, sequencing, bioinformatics) that enable the interpretation sequence data (e.g., previous examples). High-quality contextual data is also crucial for quality control. For example, detecting systematic batch effect errors related to certain sequencing centres and methods can help evaluate which variants represent real, circulating viruses, as opposed to artefacts of sample handling or sequencing which may arise due to different aspects experimental design, laboratory procedures, bioinformatics processing, and applied quality control thresholds \cite{de_maio_issues_2020, rayko_quality_2020, poon_recurrent_2005}. 

Good data stewardship practices are not only critical for auditability and reproducibility, but for posterity and can help future-proof information used to to inform a diagnosis or dealing with a public health crises. Contextual data, however, is often collected on a project-specific basis according to local needs and reporting requirements which results in the collection of different data types at different levels of granularity, with different meanings and implicit bias of variables and attributes. Furthermore, the information is often collected as free text, or if structured, according to organisation or initiative-specific data dictionaries, using different fields, terms, formats, abbreviations, and jargon.

As discussed in chapter \ref{ch:paper6}, minimum information checklists (MIxS \cite{yilmaz_minimum_2011}, MIGS \cite{field_minimum_2008}, the NIAID/BRC Project and Sample Application Standard \cite{dugan_standardized_2014}) and various interoperable ontologies (OBO Foundry \cite{smith_obo_2007}), make information easier to aggregate and reuse, but their scope . As such, a fit for purpose contextual data schema,taking into consideration ethical, practical and privacy concerns, is necessary for not only the reporting of metagenomic results, but also its dissemination through, for example, public genomic repositories. This aids not only the inference of information for diagnosis, but also public health efforts for the identification of potential outbreaks.  

\subsection{The need for Crowdsourcing}

The term ‘crowdsourcing’ is used to describe systems for accomplishing directed work that requires human intelligence. These human-powered systems are built to solve discrete tasks with clear end points \citep{good_crowdsourcing_2013}. Crowdsourcing for better standards represents a viable way to adopt better practices for the use of metagenomics in clinical microbiology.

Different forms of crowdsourcing were successfully applied to address key problems in bioinformatics \citep{good_crowdsourcing_2013, connor_ncbis_2019, powell_organizing_2021}. Chapter \ref{ch:paper7} represents a proof of concept based on our experiences with developing software tests during a hackathon organised prior to the ASM Conference on Rapid Applied Microbial Next-Generation Sequencing and Bioinformatic Pipelines (ASM NGS) in 2020. In the end, we defined a set of
seven recommendations for researchers in microbial bioinformatics looking to develop software tests. Although limited to software testing, it represents a viable form of engaging the community in order to uphold new standards.

In the application of metagenomics to clinical diagnostics, we have discussed major pitfalls that require not only expert human input, but also a wide adoption by the community. Crowdsourcing generally begins where automation fails, and this is where initiatives such as hackathons represents a fundamental change in the way that scientific work is performed and distributed within the community. Recalling the original definition, crowdsourcing is a shift from work done in-house to work done in the open by anyone that is able. This means not only that we can often solve more problems more efficiently, but also that different people are solving them. Ultimately, is through crowdsourcing that the fastest and most efficient way can be applied to change golden standards, adopt new paradigms and ensure that the necessary conditions are widely spread. 

