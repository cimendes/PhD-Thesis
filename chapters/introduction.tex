\mbox{}\\
\vspace{8cm}

\section{The global impact of microbial pathogens} \label{sec:global_impact}

The Global Burden of Disease (GBD) 2019 study reported that microbial pathogens are responsible for more than 400 million years of life lost annually across the globe, a higher burden than either cancer or cardiovascular disease \citep{vos_global_2020}. 
In particular, lower respiratory infections, diarrhoeal diseases, HIV/AIDS and tuberculosis were amongst the five leading causes of global total years of life lost. 
More recently, the COVID-19 pandemic, declared as such by the World Health Organization (WHO) on 11 March 2020 after the emergence and global spread of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and as of January 2022, has caused more than 5.63 million deaths worldwide \citep{ritchie_coronavirus_2020}, making it one of the deadliest pandemics in history.
Coronavirus has been responsible for three of the eighteen major pandemics registered throughout modern history \citep{piret_pandemics_2021}, all occurring after the year 2000.
\textit{Yersinia pestis}, responsible for three pandemics of plague, \textit{Vibrio cholerae}, with seven cholera pandemics, and Influenza A virus, the causative agent of five flu pandemics, are responsible for the remaining, with Influenza being the only other pathogen with a pandemic registered after 2000. 
Recent decades have also witnessed the emergence of additional virulent pathogens, including the Ebola virus, West Nile virus, Dengue virus and Zika virus, particularly in lower-income countries.

In addition to the emergence of virulent pathogens, the rise of antimicrobial resistance (AMR) poses a major threat to human health around the world. 
In 2019 there were an estimated 4.95 million deaths associated with bacterial AMR \citep{murray_global_2022}. In 2017, the WHO released The Global Priority Pathogens (GPP) list \citep{world_health_organization_prioritization_2017} to guide discovery, research and development of new antibiotics for drug-resistant bacterial infections (see Figure ~\ref{fig:figure1}). 
Besides tuberculosis, the global priority due to being the most common and lethal airborne AMR disease worldwide today, responsible for 250 000 deaths each year, it includes 12 groups of pathogens in three priority categories. 

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 1.pdf}
\caption{\textbf{World Health Organisation Global Priority Pathogens list.} This catalogue includes, besides \textit{Mycobacterium tuberculosis} considered the number one global priority, a list of twelve microorganisms grouped under three priority tiers according to their antimicrobial resistance: critical (\textit{Acinetobacter baumannii}, \textit{Pseudomonas aeruginosa} and \textit{Enterobacteriaceae}), high (\textit{Enterococcus faecium}, \textit{Helicobacter pylori}, \textit{Salmonella} species, \textit{Staphylococcus aureus}, \textit{Campylobacter} species and \textit{Neisseria gonorrhoeae}), and medium (\textit{Streptococcus pneumoniae}, \textit{Haemophilus influenzae} and \textit{Shigella} species). The major objective was to encourage the prioritisation of funding and incentives, align research and development priorities of public health relevance, and garner global coordination in the fight against antimicrobial resistant bacteria. Adapted from \cite{world_health_organization_prioritization_2017}.}
\label{fig:figure1}
\end{figure*}

Clinical microbiology is a discipline focused on rapidly characterising pathogen samples to direct the management of individual infected patients (diagnostic microbiology) and monitor the epidemiology of infectious disease (public health microbiology), including the detection of outbreaks and infection prevention. 
According to WHO's Global Expenditure on Health report from 2000 to 2019, of the 51 countries that reported health spending by disease and condition, an average of 37\% of health spending went to infectious and parasitic diseases, corresponding to the largest share of health spending \citep{world_health_organization_global_2021}. 
About 21\% of total health spending went to three major infectious diseases — HIV/AIDS (9\%), tuberculosis (1\%) and malaria (11\%) — and 16\% went to other infectious and parasitic diseases. 
On average, 70\% of external aid for health went to infectious and parasitic diseases in the 51 low and middle-income countries. 
Of the \$54.8 billion estimated disbursed for health in 2020, \$13.7 billion (25\%) was targeted toward the COVID-19 health response \citep{micah_tracking_2021}.

\subsection{Current standards for diagnostic in clinical microbiology} \label{ssec:current_standards}

The past few decades have seen a major revolution in the operation of microbial laboratories, driven by the development of molecular technologies and ways to make these accessible, namely amplification-based polymerase chain reaction (PCR), matrix-assisted laser desorption/ionisation - time of flight (MALDI-TOF) and DNA-microarray-based hybridisation technology. 
These are used in conjunction with traditional techniques such as microscopy, culture and serology, not fully replacing them.
Application of these methods differs by suspected infection type: bacterial, viral, fungal or parasitic. 
For the purpose of this dissertation work, we will be focusing on bacterial and viral infections.

\subsubsection{Bacterial infections} \label{sssec:bacterial}

For patients with bacterial infections, the crucial steps are (1) to grow an isolate from a specimen, (2) identify its species, and (3) determine its pathogenic potential and test its susceptibility to antimicrobial drugs  \citep{didelot_transforming_2012}. 
Together this information facilitates the specific and rational treatment of patients. 
For public health purposes, knowledge also needs to be gained about (4) the relatedness of the pathogen to other strains of the same species to investigate transmission routes and enable the recognition of outbreaks \citep{foxman_choosing_2005} (see Figure \ref{fig:figure2}). 

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 2.pdf}
\caption{\textbf{Principles of current processing of bacterial pathogens.} Schematic representation of the current workflow for processing samples for bacterial pathogens is presented, with high complexity and a typical timescale of a few weeks to a few months. Samples that are likely to be normally sterile are often cultured on rich medium that will support the grown of any culturable organism. Samples contaminated with colonising flora present a challenge for growing the infecting pathogen. Many types of culture media (referred to as selective media) are used to favour the growth of the suspected pathogen.
Once an organism is growing, the likely pathogens are then processed through a complex pathway that has many contingencies to determine species and antimicrobial susceptibility. Broadly, there are two approaches. One approach uses MALDI-TOF for species identification prior to setting up susceptibility testing. The other uses Gram staining followed by biochemical testing to determine species; susceptibility testing is often set up simultaneously with doing biochemical tests. Lastly, depending on the species and perceived likelihood of an outbreak, a small subset of isolates may be chosen for further investigation using a wide range of typing tests. Adapted from \cite{didelot_transforming_2012}}
\label{fig:figure2}
\end{figure*}

The current gold standard for bacterial pathogen identification in diagnostic microbiology laboratories involves the isolation of the pathogen through culture followed by biochemical testing, a multi-step process that can take days to weeks before obtaining results, depending on the fastidiousness of the organism and if it can be cultured \citep{muhamad_rizal_advantages_2020, giuliano_guide_2019, muhamad_rizal_advantages_2020}. 
Although culture allows the identification of a wide variety of organisms, some pathogens can escape routine investigation due to strict metabolic necessities for growth or the requirement for specific biochemical tests needed for their identification. 
Additionally, results will be obscured if a mixed culture is obtained, particularly if the cultures are obtained from sites with a microbiota, such as the gut and the skin, increasing the risk of contamination from normal flora, and leading to false results \citep{giuliano_guide_2019}. 
After successful growth in culture, Gram staining and MALDI-TOF mass spectrometry are often used for identification with good accuracy as long as the pathogen is presented in the the coexisting database \citep{patel_maldi-tof_2015}. 
An alternate rapid identification method is PCR where nucleic acid fragments are detected through specific primers, being highly sensitive and specific, to the point where PCR may detect bacteria that are not viable after a patient has been treated for an infection and it is limited to the primer used \citep{scerbo_beyond_2016}. 
Syndromic panels, an extension of PCR by using multiple primers (multiplex PCR) to simultaneously amplify the nucleic acids of multiple targets in a single reaction, tried to address this issue by allowing for the identification of multiple bacteria and other important information such as the detection of antibiotic resistance or virulence genes \citep{giuliano_guide_2019}

Following identification, antibiotic-susceptibility testing is essential for guiding clinicians in selecting an appropriate treatment. 
Conventional detection methods of bacterial resistance, such as disc diffusion, antimicrobial gradient strip and broth microdilution, are widely used but results cannot be obtained earlier than 48 hours after receiving a sample, which may lead to prolonged use or overuse of broad-spectrum antibiotics \citep{benkova_antimicrobial_2020}. 
Similarly to bacterial identification, MALDI-TOF and PCR have been increasingly adopted as solutions with lower turnaround times, although no phenotypic information is retrieved, nor information on the minimum inhibitory concentration (MIC) for a given antibiotic.   

Choosing an appropriate bacterial typing technique for epidemiological studies depends on the resources available and the minimum intended resolution, ranging from DNA fingerprinting to multilocus sequence typing, Pulsed-field gel electrophoresis (PFGE) and sequence-based typing (see \secref{sec:genomics_approach}) \citep{allerberger_molecular_2012,foxman_choosing_2005}. 
DNA macrorestriction analysis by PFGE, which revolutionised precise separation of DNA fragments, became the most widely implemented DNA fingerprinting technique \citep{allerberger_molecular_2012}, becoming the golden standard for bacterial typing \citep{neoh_pulsed-field_2019}.

In the early 2000s, Multilocus sequence typing (MLST) was proposed as a portable, universal, and definitive method for characterising bacteria \citep{maiden_multilocus_2006}. 
Instead of enzyme restriction of bacteria DNA, separation of the restricted DNA bands using a PFGE chamber, followed by clonal assignment of bacteria based on banding patterns, MLST relies on the amplification through PCR sequences of internal fragments of housekeeping genes (usually 5 to 7), approximately 450-500 basepairs (bp) in size, followed by its the sequence, usually my Sanger methods (see \secref{ssec:1st_gen_seq}). 
For each house-keeping gene, the different sequences present within a bacterial species are assigned as distinct alleles and, for each isolate, the alleles at each of the (usually) seven loci define the allelic profile or sequence type \citep{larsen_multilocus_2012}. 
As with PFGE, different schemes, defining what house-keeping gene fragments are used, are available depending on the species. 
Unlike PFGE, the provision of freely accessible, curated databases of MLST nucleotide sequence data enables the direct comparison of bacterial isolates, providing the basis of a common language for bacterial typing \citep{maiden_multilocus_2006}. 
So far, MLST schemes for 115 bacterial organisms have been published and made freely available\footnote{\url{https://pubmlst.org/organisms}}, \cite{jolley_open-access_2018}) 

Depending on the organism identified, further and/or particular typing schemes can be applied. 
For \textit{S. pneumoniae}, one of the pathogens listed in the WHO's GPP list, the typing of the polysaccharide capsule, usually through Quellung reaction, is paramount for disease surveillance and pre- and post-pneumococcal vaccine evaluation as the capsule, with over 90 serotypes reported, is the dominant surface structure of the organism and plays a critical role in virulence \citep{jauneikaite_current_2015, paton_streptococcus_2019}. 
For the \textit{Salmonella} species, also in the GPP list, the serotype is usually determined by agglutination of the bacteria with specific antisera to identify variants of somatic (O) and flagella (H) antigens that, in various combinations, characterise more than 2600 reported serotypes \citep{diep_salmonella_2019}. 

\subsubsection{Viral infections} \label{sssec:viral}

The traditional approaches to the laboratory diagnosis of viral infections have been (1) direct detection in patient material of virions, viral antigens, or viral nucleic acids, (2) isolation of the virus in cultured cells, followed by identification of the isolate, and (3) detection and measurement of antibodies in the patient’s serum (serology) \citep{burrell_laboratory_2017}. 
Viral diagnostics is therefore generally organised into two primary categories, indirect and direct detection, depending on the method used. 

Indirect detection methods involve the propagation of virus particles via their introduction to a suitable host cell line (virus isolation), as viruses rely on host organisms to replicate. 
This is a relatively slow diagnostic method, sometimes taking weeks for the virus to propagate, usually followed by microscopy for its identification, or more commonly, through molecular methods with an agent which detects a virus-associated protein, such as an antibody \citep{cassedy_virus_2021}. 

Direct detection methods negate the need for virus propagation, detecting the virus directly from the suspect source through nucleic acid and immunological methods. 
PCR and reverse transcription-PCR (RT-PCR) are widely applied methods for the detection of both DNA and RNA viruses, respectively, driven by increased awareness of the clinical value of, and demand for, prompt information about viral loads, viral sequence data, and potential antiviral resistance information \citep{cassedy_virus_2021}. 
Syndromic testing (see \secref{sssec:bacterial}) is now fully integrated into the standard testing practices of many clinical laboratories \citep{dien_bard_panels_2020}. 
Limitations of these assays include no detection of off-target pathogens, a lack of full susceptibility information, cost, and false-positive results. 
Real-time quantitative PCR (qPCR) remains the front line tool in aetiological diagnosis, measuring the production of the target amplicon throughout the reaction and providing quantitative results with high specificity and sensibility, albeit with a significant cost due to sophisticated apparatus despite high-throughput systems being widely established \citep{cassedy_virus_2021}.

Immunoassays employ singular-epitope specificity antibodies as the primary means to detect viruses within a sample and provide a much more cost-efficient alternative to nucleic acid detection \citep{cassedy_virus_2021}. 
One major application is seroprevalence assays, an essential technique for identifying patients who have been exposed to a virus (historical exposure), detecting asymptomatic infection or evaluating vaccine efficacy  \citep{chan_determining_2021, bobrovitz_global_2021}. 
Lateral flow immunoassays (LFIA) are extensively used for detecting virus-associated protein directly from the source through labelled antibodies binding to their cognate antigens, usually read by way of a colour change at a test line. 
Besides being very cost-effective, LFAIs have a turnaround time of minutes and the colour change can be observed with the naked eye, therefore facilitating rapid diagnosis but its results are limited to semi-quantitative and it does not typically achieve sensitivity comparable to nucleic-acid detection \citep{estrela_lateral_2016, cassedy_virus_2021, di_nardo_ten_2021}.

\subsection{Surveillance and infection prevention in public health} \label{ssec:survaillance}

Infectious disease surveillance is critical for improving population health, generating information that drives action not only in the management of infected patients but also in the prevention of new ones by identifying emerging health conditions that may have a significant impact by (1) describing the current burden and epidemiology of the disease, (2) to monitoring trends, and (3) identifying outbreaks and new pathogens \citep{groseclose_public_2017, murray_infectious_2017}. 
Public health surveillance systems (PHSS) are composed of the ongoing systematic collection, analysis, and interpretation of data, and its integration with the timely dissemination of results to those who can undertake effective prevention and control activities \citep{teutsch_considerations_2010}. 

Traditional PHSS can have different approaches based on the epidemiology and clinical presentation of the disease and the goals of surveillance. 
In passive surveillance systems, medical professionals in the community and at health facilities report cases to the public health agency, which conducts data management and analysis once the data are received and communicate with the responsible entities. 
Globally, the WHO as described in the International Health Regulations what is notifiable by every country to WHO, such as Severe acute respiratory syndrome (SARS) and Viral haemorrhagic fevers (Ebola, Lassa, Marburg), as well as guiding what public health measures should be implemented \citep{world_health_organization_international_2005}. 
Active surveillance aims to detect every case, not relying on a reporting structure, and can have many approaches from sentinel sites or network of sites that capture cases of a given condition, such as respiratory tract infections, within a catchment population \citep{murray_infectious_2017, melo-cristino_estudo_2006}. 
The application of environmental surveillance methods, performed prospectively to detect pathogens prior to the recording of clinical cases or to monitor their abundance in the environment to assess the potential risk of disease, has been proven as a viable alternative, particularly in wastewater \citep{andrews_environmental_2020, mcweeney_demonstration_1894, baker_combined_2011, larsen_tracking_2020}.  

The emergence and re-emergence of infectious diseases are closely linked to the biology and ecology of infectious agents, their hosts, and their vectors \citep{destoumieux-garzon_one_2018}.
"One Health" is a collaborative and multi-disciplinary approach to designing and implementing programmes, policies, legislation and research in which multiple sectors communicate and work together to achieve better public health outcomes \citep{mackenzie_one_2019}. 
It recognises that people’s health is closely connected to animals’ health and shared environment, focusing on zoonotic and vector-borne diseases, antimicrobial resistance, food safety, food security and environmental contamination \citep{rugarabamu_one-health_2021}.
This is crucial to (1) understanding the emergence and re-emergence of infectious and non-communicable chronic diseases and (2) in creating innovative control strategies.
A better knowledge of causes and consequences of certain human activities, lifestyles, and behaviours in ecosystems is crucial for a rigorous interpretation of disease dynamics and to drive public policies, but it requires breaking down the interdisciplinary barriers that still separate human and veterinary medicine from ecological, evolutionary, and environmental sciences \citep{destoumieux-garzon_one_2018}. 

\section{A genomic approach to clinical microbiology} \label{sec:genomics_approach}

Since the publication of the first complete microbial genome a quarter of a century ago, that of the bacterium \textit{Haemophilus influenzae} \citep{hood_dna_1996}, genomics has transformed the field of microbiology, and in particular its clinical application (see Figure \ref{fig:figure3}). 

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 3.pdf}
\caption{\textbf{Principles of current processing of bacterial pathogens based on whole genome sequencing.} Schematic representation of the workflow for processing samples for bacterial pathogens after adoption of whole genome sequencing, with an expected timescale that could fit within a single day. The culture steps would be the same as currently used in a routine microbiology laboratory. Once a likely pathogen is ready for sequencing, DNA will be extracted, taking as little as 2 hours to prepare the DNA for sequencing.
After sequencing, the main processes for yielding information will be computational. Automated sequence assembly algorithms are necessary for processing the raw sequence data, from which species, relationship to other isolates of the same species, antimicrobial resistance profile and virulence gene content can be assessed. All the results will also be used for outbreak detection and infectious diseases surveillance. Adapted from \cite{didelot_transforming_2012}}
\label{fig:figure3}
\end{figure*}

The paper describing the DNA-sequencing method with chain-terminating inhibitors used in the sequencing of the first microbial genome \citep{sanger_dna_1977}, which earned the late Frederick Sanger his share of the 1980 Nobel Prize in Chemistry alongside Walter Gilbert, was, in 2014, the top fourth in the number of citations with 60335, highlighting its impact in the field of biological sciences, and by extension medicine \citep{van_noorden_top_2014}. 
Currently, this number has increased to 84546 according to PubMed Central\textsuperscript{\small\textregistered} (PMC)\footnote{\url{https://pubmed.ncbi.nlm.nih.gov/}}\footnote{\url{ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC431765/}}. 
Since its emergence, reductions in cost, technical advances in sequencing technologies and new computational developments have made genomic sequencing one of the most influential tools in biomedical research, yielding unprecedented insights into microbial evolution and diversity, and the complexity of the genetic variation in both commensal and pathogenic microbes. 
The emerging application of genomic technologies in the clinic to combat infectious diseases is transforming clinical diagnostics and the detection and surveillance of outbreaks. 

\subsection{Twenty five years of microbial genome sequencing} \label{ssec:sequencing}

Since the discovery of the structure of DNA \citep{watson_molecular_1953}, great strides have been made in understanding the complexity and diversity of genomes in health and disease. 
The development and commercialisation of high-throughput, massively parallel sequencing, has democratised sequencing by offering individual laboratories, either in research or in health, access to the technology. 
Over the last quarter of a century, three main revolutions can be considered in genomic sequencing (see Figure ~\ref{fig:figure5}).

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 5.png}
\caption{\textbf{The three revolutions in sequencing technology that have transformed the landscape of bacterial genome sequencing.} The first-generation, also known as Sanger sequencers, is represented by the ABI Capillary Sequencer (Applied Biosystems). During the sequencing reaction, at each nucleotide incorporation event a fluorescently labelled ddNTP is incorporated, terminating the elongation of the DNA molecule. The resulting electropherogram for sequencing reaction is below, and is read from left to right. The second-generation, also known as high-throughput sequencers, is represented by the MiSeq, a 4-channel sequencer, and the NextSeq, a 2-channel sequencer (Illumina), both sequencing by synthesis instruments. For both instruments, the loaded flowcell is sequenced in massive parallel reactions, with each nucleotide incorporation emitting a light signal that is captured and latter basecalled into a fastq file, with indication of the confidence of the call, presented bellow. In a 4-channel instrument each nucleotide has it's own marker (A: yellow, T: green, C: red, G: blue) but in a 2-channel instrument only 2 markers exist (A: green plus red, T: green, C: red, G: no marker). These instrument allow the sequencing of both ends of the DNA fragment. Lastly, the third-generation, also known as long-read sequencers, is represented by Pacific Bioscience BS sequencer and Oxford Nanopore MinION sequencer. In the first, immobilised polymerases in a SMRT Cell incorporating nucleotides with identifying fluorescent labels. In the latter, a nanopore embedded in a solid-state membrane causes a a change in an ionic current across the membrane each time a nucleotide is pushed though the pore. This difference in potential is then used for basecalling. Adapted from \cite{hagemann_overview_2015, loman_twenty_2015,goodwin_coming_2016, wang_nanopore_2021, metzker_sequencing_2010, xu_recent_2020}}
\label{fig:figure5}
\end{figure*}

\subsubsection{The first-generation of DNA sequencing} \label{ssec:1st_gen_seq}

In the late 1980s, automated Sanger sequencing machines could sequence approximately 1,000 bases per day, having been applied in the 1990s to large bacterial genomes and the first unicellular and multicellular eukaryotic genomes, including the completion of a high-quality, reference sequence of the human genome under the Human Genome Project (HGP) \citep{koch_sequencing_2021, collins_human_1995}. 
The first genomes of the pathogenic \textit{Mycobacterium tuberculosis} \citep{cole_deciphering_1998}, \textit{Yersinia pestis} \citep{parkhill_genome_2001}, \textit{Escherichia coli} K-12 \citep{blattner_complete_1997} were sequenced using this technology, requiring years of effort and significant budgets but providing insights into the genomic complexity of these organisms. 
Some of the complete genome sequences produced during this era are still used today as high-quality references. 

Simplistically, in Sanger sequencing, also known as “first-generation” DNA sequencing, a DNA polymerase is used to synthesize numerous copies of the sequence of interest using dideoxynucleotide triphosphates (ddNTPs) spiked into the reaction. 
At each nucleotide incorporation event, there is a chance that a ddNTP will be added and the growing DNA chain will be terminated, resulting in a collection of DNA molecules of varying lengths \citep{sanger_dna_1977, hagemann_overview_2015}. 
Modern Sanger sequencing uses fluorescently labelled ddNTPs that allow the amplification step to be performed in a single reaction, resulting in a mixture of single-stranded DNA fragments of various lengths, each tagged at one end with a fluorophore indicating the identity of the 3' nucleotide that, after separation through capillary electrophoresis, the resulting electropherogram with four-colour fluorescence intensity can be interpreted by a base-calling software and producing 600–1000 bases of accurate sequence \citep{hagemann_overview_2015}. 

The Sanger sequencing technology remains very useful for applications where high-throughput is not required due to its cost-effectiveness, relatively low sample load and accuracy of sequencing even in repetitive genomic regions, although input DNA must consist of a relatively pure population of sequences \citep{slatko_overview_2018}. 
One of the most common uses is thus individual sequencing reactions using a specific DNA primer on a specific template, such as MLST of bacterial genomes. 

\subsubsection{The second-generation of DNA sequencing} \label{ssec:2nd_gen_seq}

The release of the first truly high-throughput sequencing platform in the mid-2000s heralded a 50,000-fold drop in the cost of DNA sequencing in comparison with the first-generation technologies and led to the denomination of next-generation sequencing (NGS) \citep{goodwin_coming_2016}. 
This trend has continued throughout the next two decades of continued development and improvement, allied to the emergence of benchtop sequencing platforms with a high-throughput of sequencing data and turnaround times of days, making it a standard in any microbiology and public health laboratories \citep{loman_twenty_2015}. 
Second-generation sequencing methods can be grouped into two major categories: (1) sequencing by hybridisation and (2) sequencing by synthesis. 

\paragraph{Sequencing by hybridisation} \label{sssec:2nd_gen_seq_hybrid} \mbox{}\\

Sequencing by hybridisation, also known as sequencing by ligation, originally developed in the 1980s, relies on the binding of one strand of DNA to its complementary strand (hybridisation). 
By repeated hybridisation and washing cycles, it was possible to build larger contiguous sequence information, based upon overlapping information from the probe hybridisation spot, being sensitive to even single-base mismatches when the hybrid region is short or if specialised mismatch detection proteins are present \citep{slatko_overview_2018, detter_nucleic_2014}. 
Although widely implemented via DNA chips or microarrays, has largely been displaced by other methods, including sequencing by synthesis \citep{goodwin_coming_2016}. 

\paragraph{Sequencing by synthesis} \label{sssec:2nd_gen_seq_synth} \mbox{}\\

Sequencing by synthesis methods are a further development of Sanger sequencing, without the ddNTPs terminators, in combination with repeated cycles, run in parallel, of synthesis, imaging, and methods to incorporate additional nucleotides in the growing chain. 
All second-generation sequencing by synthesis approaches relies on a ‘library’ preparation using native or amplified DNA usually obtained through (1) DNA extraction, (2) DNA fragmentation and fragment size selection, and (3) ligation of adapters and optional barcodes to the ends of each fragment. 
This is generally followed by a step of DNA amplification. The resulting library is loaded on a flow cell and sequenced in massive parallel sequencing reactions \citep{giani_long_2020}
Besides having much shorter read lengths than first-generation methods, with reads ranging from 45 to 300 bases, and an intrinsically higher error rate, the massively parallel sequencing of millions to billions of short DNA sequence reads allows for the obtainment of millions of accurate sequences based upon the identification of a consensus (agreement) sequences \citep{slatko_overview_2018, goodwin_coming_2016, hagemann_overview_2015}. 

Many of the currently available sequencing by synthesis methods approaches have been described as cyclic array sequencing platforms, as they involve dispersal of target sequences across the surface of a two-dimensional array, followed by sequencing of those targets \citep{hagemann_overview_2015}. 
They can be further classified as either single-nucleotide addition or cyclic reversible termination or as single-nucleotide addition \citep{goodwin_coming_2016}. 

The first relies on a single signal to mark the incorporation of a dNTP into an elongating strand, avoiding the use of terminators. 
As a consequence, each of the four nucleotides must be added iteratively to a sequencing reaction to ensure only one deoxynucleotide triphosphate (dNTP) is responsible for the signal. 
The Roche 454 Life Sciences pyrosequencing device \footnote{\url{https://web.archive.org/web/20161226040638/http://454.com/}, snapshot from 26 December 2016}, was the first and most popular instrument implementing this technology, but discontinued since 2013 with support to the platform ceasing since 2016. 
This system distributes template-bound beads into a PicoTiterPlate along with beads containing an enzyme cocktail. 
As a dNTP is incorporated into a strand, an enzymatic cascade occurs, resulting in a bio-luminescence signal which is captured by a camera, which can be attributed to the incorporation of one or more identical dNTPs at a particular bead \citep{goodwin_coming_2016}. 
The ThermoFisher Ion Torrent system \footnote{\url{https://www.thermofisher.com/pt/en/home/brands/ion-torrent.html}}, released in 2010 and still available today, replaces the optical sensor, using instead  H+ ions that are released as each dNTP is incorporated in the enzymatic cascade, and the consequential change in pH, to detect a signal \citep{goodwin_coming_2016}. 
Alongside the 454 pyrosequencing system, this system has difficulty in enumerating long repeats, additionally, the throughput of the method depends on the number of wells per chip, ranging from 10 megabases to 1000 megabases of 100 base reads in length, but with a very short run time (three hours) \citep{hagemann_overview_2015, loman_performance_2012}.

The latter is defined by their use of terminator molecules that are similar to those used in the first-generation of sequencing, preventing elongation of the DNA molecule, but unlike the first methods, it is reversible. 
To begin the process, a DNA template is primed by a sequence that is complementary to an adapter region, which will initiate polymerase binding to this double-stranded DNA region. 
During each cycle, a mixture of all four individually labelled and 3'-blocked dNTPs are added. 
After the incorporation of a single dNTP to each elongating complementary strand, unbound dNTPs are removed and the surface is imaged to identify which dNTP was incorporated at each cluster by optical capture. 
The fluorophore and blocking group can then be removed and a new cycle can begin \citep{goodwin_coming_2016}. 
The Illumina systems, which use this technology, accounts for the largest market share for sequencing instruments compared to other platforms\footnote{\url{https://www.forbes.com/companies/illumina/?sh=774358a91aa6}}, allowing paired-end sequencing and having the highest throughput (from 25 million reads for a MiSeq instrument to  1.2 billion reads for a NextSeq instrument\footnote{\url{https://www.illumina.com/systems/sequencing-platforms.html}}), with read lengths ranging from 45 to 300 bases in length with high accuracy, albeit with long running times (4 to 55 hours), rendering this technology a good choice for many sequencing applications where large read length is not required \citep{loman_performance_2012, gupta_chapter_2014, hagemann_overview_2015}.

\subsubsection{The third-generation of DNA sequencing} \label{ssec:3rd_gen_seq}

Despite their wide adoption, second-generation methods require library preparation and an enrichment or amplification step. 
These steps are time-consuming, introduce biases related to preferential capture or amplification of certain regions, and produce reads with relatively small size, making transversing repetitive genomic regions impossible if they are larger than the read length \citep{hagemann_overview_2015}. 
Third-generation sequencing technologies, also known as long-read sequencing or single-molecule sequencing, are characterised by the generation of ultra-long-reads, albeit at a much lower throughput than the second-generation \citep{hoang_long-reads-based_2022}. 
They also have the potential to go beyond four-base sequencing to reveal genome-wide patterns of methylation and other chemical modifications that control the biology of bacteria or the virulence of pathogens \citep{korlach_going_2012}. 
Currently, commercial long-read sequencing is supported by two companies: Pacific Biosciences\footnote{\url{https://www.pacb.com/}} and Oxford Nanopore Technologies\footnote{\url{https://nanoporetech.com/}}. 

The basis of Pacific Biosciences sequencers is known as single-molecule real-time sequencing (SMRT), which takes place in single-use SMRT Cells. 
These contain multiple immobilised polymerases which, after binding to an adaptor sequence, begins replication incorporating nucleotides with identifying fluorescent labels. 
The sequence of fluorescence pulses is recorded into a movie which is then converted into a nucleotide sequence. 
After the polymerase completes replication of one DNA strand, it continues to sequence the opposite adapter and second strand. 
As a result, it is possible to generate multiple passes of the same template depending on the lifetime of the polymerase \citep{hoang_long-reads-based_2022, loman_twenty_2015}. 
This technology has accuracy comparable with the Illumina systems but requires a higher initial investment cost, are much larger machines in comparison with the benchtop counterparts, and have much lower throughput and longer library preparation protocols \citep{hoang_long-reads-based_2022, wenger_accurate_2019}. 

Oxford Nanopore Technologies makes use of nanopores in small, portable single-molecule sequencing devices, capable of generating ultra-long sequences in real-time at a relatively low cost. 
Biological nanopores are embedded in solid-state membranes within disposable flow cells which, when a DNA strand passes through the pore driven by a motor protein, each nucleotide causes a change in an ionic current across the membrane, which is later base called \citep{hoang_long-reads-based_2022, loman_twenty_2015}. 
This process is free from fluorescence labels and amplification requirements, and after one strand is processed, the pore is available to sequence the next available strand. 
Sequence quality and length depend on the loaded library but are usually much lower than the alternative counterparts, and its throughput is dependent on the number and lifespan of the nanopore within the flowcell, but still much lower than the alternatives. 
Despite this, its portability, fast advances, and continued improvement of the flowcells make this a fast adopted technology for long-read sequencing.  

\subsection{DNA sequencing in clinical diagnosis and surveillance} \label{ssec:sequencing_diagnosis}

Whole-genome sequencing (WGS) is becoming one of the most widely used applications of microbial genome sequencing. 
The major advantage of WGS is to yield all the available DNA information content on isolates in a single rapid step following culture (sequencing without culture will be discussed in the \secref{ssec:metagenomics}). 
In principle, after obtaining a pure culture, either bacterial (see \secref{sssec:bacterial}) or viral (see \secref{sssec:viral}), the data from sequencing contain all the information currently used for diagnostic and typing needs, and much more, thus opening the prospect for large-scale research into pathogen genotype-phenotype associations from routinely collected data \citep{didelot_transforming_2012}.
The cost of producing massive amounts of information requires a new framework with expert handling and processing of computer-driven genomic information, as well as capable computational infrastructures, but through this technology, researchers and clinicians can obtain the most comprehensive view of genomic information and associated biological implications, transforming clinical diagnostics and the detection and surveillance of outbreaks. \citep{cirulli_uncovering_2010, nature_reviews_genetics_genomic_2019, goodwin_coming_2016}.

Targeted sequencing is also proving invaluable to clinical microbial and research, not only by allowing more individual samples to be sequenced within a single run, significantly reducing costs and the amount of data generated, but also, due to the smaller target size, obtaining results with very high confidence due to the high coverage obtained \citep{goodwin_coming_2016}.
This has been particularly useful in viral genomics where sections, such as the capsid, or the complete viral genome can be selectively targeted directly from the suspected sample, offering a more time-effective method to achieve the same output as traditional nucleic acid amplification methods \citep{cassedy_virus_2021}. 

\subsubsection{Sequencing in the routine laboratory workflow} \label{sssec:sequencing_routine_lab}

WGS has been used in the routine laboratory workflow when typing of pathogens by a method having the highest possible discriminatory power is required either through single nucleotide polymorphism (SNP) or core-genome/whole genome MLST (cg/wg MLST) analysis, for example during hospital outbreaks \citep{tagini_bacterial_2017}. 
Additionally, in bacterial diagnostics, WGS can be used to reveal the presence of AMR genes, or genes associated with virulence and pathogenicity, as well as to discover new genetic mechanisms for the three previously defined important clinical features of a bacterium \citep{rossen_practical_2018}. 
The implementation of WGS in routine diagnostics requires several adaptations in the laboratory workflow, from the ‘wet’ laboratory part (extraction, library preparation, sequencing), to the ‘dry’ bioinformatics part where genomic data is analysed and its results interpreted by specialised personnel \citep{rossen_practical_2018}. 

Currently, sequencing technologies are used in a case-by-case approach, with its adoption being much more present in a research setting than in a diagnostic one. 
Sequencing is mostly used after a diagnostic through the identification of the causative agent has already been performed. 
Although substantial advances have been made in reducing response time, most of the current systems do not yet generate enough data fast enough for a truly rapid response for it to be used in the clinical setting \citep{goodwin_coming_2016}. 
High-throughput DNA sequencing has found additional new applications in drug discovery and in functional genomics with, for example, SNP-based analysis to identify new drug targets \citep{loman_twenty_2015}.

Although the second-generation DNA sequencing methods have shed light on fundamental aspects of microbial ecology and function, they suffer from issues associated with short read length (see \ref{ssec:2nd_gen_seq}) and cannot reliably reconstruct long repeats because of uncertainties in mapping read, even when paired-end sequencing is used. 
Third-generation sequencing methods (see \ref{ssec:3rd_gen_seq}) have become increasingly used in microbiology, although their accuracy and low throughput make it challenging to implement in a clinical diagnostic setting. 

\subsubsection{Sequencing and genomic surveillance} \label{sssec:sequencing_genomic_survaillance}

Most notably, WGS has become a common tool in surveillance and infection prevention, allowing for pathogen identification and tracking, establishing transmission routes and outbreak control \citep{lo_genomics_2020}. 
In bacterial infections, initiatives such as Pathogenwatch\footnote{\url{https://pathogen.watch/}} offers a web-based platform for AMR analysis and phylogeny generation of \textit{Campylobacter}, \textit{Klebsiella}, \textit{Neisseria gonorrhoeae}, \textit{Staphylococcus aureus}, and \textit{Salmonella Typhi} \citep{afolayan_overcoming_2021}. 
The Center for Genomic Epidemiology website\footnote{\url{https://www.genomicepidemiology.org/}} offers services for phylogenetic tree building and AMR prediction. 
Chewie Nomenclature Server\footnote{\url{https://chewbbaca.online/}} allows users to share genome-based gene-by-gene typing schemas and to maintain a common nomenclature, simplifying the comparison of results \citep{mamede_chewie_2021}. 
Enterobase\footnote{\url{https://enterobase.warwick.ac.uk/}} allows for the analysis and visualisation of genomic variation within enteric bacteria \citep{zhou_enterobase_2020}. 
Microreact\footnote{\url{https://microreact.org/}}, from the same developers as Pathogenwatch, combines clustering, geographical and temporal data into an interactive visualisation with trees, maps, timelines and tables for a multitude of microorganisms, both bacterial and viral \citep{argimon_microreact_nodate}. 
Particularly for viruses, GISAID\footnote{\url{https://www.gisaid.org/}}  promotes the rapid sharing of data from all influenza viruses and the coronavirus causing COVID-19, including the genetic sequences and related clinical and epidemiological data \citep{shu_gisaid_2017}. 
ViPR\footnote{\url{https://www.viprbrc.org/}} provides access to sequence records, gene and protein annotations, immune epitopes, 3D structures, host factor data, and other data types for over 14 viral families, including \textit{Coronaviridae}, from which SARS-CoV-2 belongs to, and \textit{Faviviridae}, the family of Dengue and Zika virus \citep{pickett_virus_2012}. 
INSaFLU\footnote{\url{https://insaflu.insa.pt/}} supplies public health laboratories and influenza researchers with a web-based suite for effective and timely influenza and SARS-CoV-2 laboratory surveillance, identifying the type and subtype/lineage, detection of putative mixed infections and intra-host minor variants \citep{borges_insaflu_2018}. 
Nextrain\footnote{\url{https://nextstrain.org/}} provide a continually-updated view of publicly available data alongside powerful analytic and visualisation tools o aid epidemiological understanding and improve outbreak response for 10 pathogens: Influenza, SARS-CoV-2, West Nile virus, Mumps, Zika, West African Ebola, Dengue, Measles, Enterovirus D68 and Tuberculosis \citep{hadfield_nextstrain_2018}

In outbreak detection and surveillance, genetic sequencing techniques combined with epidemiological data have undoubtedly provided immeasurable insights regarding evolutionary relationships and transmission pathways in various environments \citep{beckett_pandemic_2021, lancet_genomic_2021}. 
In a pandemic setting, this approach, although not novel, has been revolutionary, particularly in the COVID-19 setting. 

In the 2009 swine-origin Influenza A H1N1 pandemic, the first complete genome was publicly available on the 25 of April of 2009 (GenBank accession number FJ966079), about a month after records of increased flu activity in Mexico and 10 days after the first confirmed cases in California, United States of America \citep{smith_origins_2009, novel_swine-origin_influenza_a_h1n1_virus_investigation_team_emergence_2009}. 
By the time the pandemic was declared, on 11 of June of 2009, \cite{smith_origins_2009} reported the origins and evolutionary genomics of the pandemic influenza A variant with a collection of 813 complete influenza genome sets, 17 of which belonging to the newly swine influenza viruses (GenBank accessions numbers GQ229259–GQ229378). 
The MERS pandemic, declared as such in 2015 \citep{piret_pandemics_2021}, had its first publicly available sequence on 5 of July 2015 (GenBank accession number KT006149)\citep{lu_complete_2015}, with a sequence from a camel, thought to be an intermediate host for the virus, available as early as 7 of March 2016 (GenBank accession number KU740200) \citep{kandeil_complete_2016, al-shomrani_genomic_2020}. 

The SARS-CoV-2 has brought a new meaning to genomic surveillance, with the first sequence from a COVID-19 patient being made publicly available as early as 12 January 2020 from a case of respiratory disease from the Wuhan outbreak (GenBank accession number MN908947) \citep{wu_new_2020}. 
At the date of the pandemic declaration by WHO, at 11 March 2020, over 400 complete SARS-CoV-2 sequences were deposited on GISAID\footnote{\url{http://web.archive.org/web/20200311053731/https://www.gisaid.org/}}, hitting over one million sequences in April 2021 \citep{maxmen_one_2021}. 
Currently, over 8 million complete viral sequences are available at GISAID\footnote{\url{https://www.gisaid.org/}}, being one of the most highly sequenced genomes of any organism on the planet. 
This richness in genomic information has been basal to identifying new variants of risk and new variants of concern with a myriad of different origins, identifying routes of transmission across borders, including the identification of "super-spreaders" events, and informing infection control measures \citep{lancet_genomic_2021, beckett_pandemic_2021, borges_sars-cov-2_2022}.  

\subsection{From genomics to metagenomics} \label{ssec:metagenomics}

Despite the increasing adoption of DNA sequencing methods in clinical microbiology, the sequencing of genetic material from a pure culture requires \textit{a priori} knowledge of what to expect from a particular clinical sample or patient \citep{schuele_future_2021}. 
In most cases, this knowledge is enough to request the most appropriate test, such as multiplexed panels or specific culture media, but this is not always the case. 
In recent years, there has been a growing interest in using metagenomics to deliver culture-independent approaches to microbial ecology, surveillance and diagnosis (see Figure \ref{fig:figure4})\citep{loman_twenty_2015, loman_high-throughput_2012}.
Metagenomic DNA sequence allows detailed characterisation of pathogens in all kinds of samples originating from humans, animals, food and the environment, ligating the diagnostics to surveillance in a true "one health" fashion \citep{rossen__2018}. 
Unlike PCR or microarrays, it usually does not require primer or probe design, it can be easily multiplexed, and the specificity and selectivity of the sequencing can be adjusted computationally after acquiring the data \citep{dunne_next-generation_2012}.  
While most molecular assays target only a limited number of pathogens, metagenomic approaches characterise all DNA or RNA present in a sample, enabling analysis of the entire microbiome as well as the human host genome or transcriptome in patient samples \citep{chiu_clinical_2019}. 
Whether or not it can entirely replace routine microbiology depends on several conditions and future developments, both technological and computational (see \secref{sec:bioinformatics}).

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 4.pdf}
\caption{\textbf{Hypothetical workflow based on metagenomic sequencing.} Schematic representation of the hypothetical workflow for the direct processing of samples from suspected sources of pathogens after adoption of metagenomic sequencing, with an expected timescale that could fit within a single day. Adapted from \cite{didelot_transforming_2012}}
\label{fig:figure4}
\end{figure*}


Albeit lacking consensus in the field, metagenomics can be classified into two variants as proposed by \citep{marchesi_vocabulary_2015}: (1) metaxonomics where marker genes ubiquitous in many taxa are targeted and sequenced, and (2) the untargeted "shotgun" sequencing of all microbial genomes present in a sample. 

\subsubsection{Metataxonomics and Targeted Metagenomics} \label{sssec:metataxonomics}

Molecular barcoding approaches can be combined with second-generation high-throughput sequencing to achieve unprecedented depths of coverage in microbial community profiling, being defined as metataxonomics. 
For profiling bacterial species, the most popular approach is 16S ribosomal RNA (rRNA) gene sequencing, an ~1500 base pair gene coding for a catalytic RNA that is part of the 30S ribosomal subunit. 
Traditionally, the variable regions of the 16S rRNA gene (V-regions) are targeted, or ranges thereof (V1-V2, V1-V3, V3-V4, V4, V4-V5, V6-V8, and V7-V9), and are specific to bacterial genus (96\%) and for some, even species (87.5\%), \citep{srinivasan_use_2015, abellan-schneyder_primer_2021}. 
Moreover, dedicated 16S databases that include near full length sequences for a large number of strains and their taxonomic placements exist, such as RDP\footnote{\url{http://rdp.cme.msu.edu/}}, Greengenes\footnote{\url{https://greengenes.secondgenome.com/}}, silva\footnote{\url{https://www.arb-silva.de/}} and NCBI's 16S ribosomal RNA project\footnote{\url{https://www.ncbi.nlm.nih.gov/refseq/targetedloci/}} \citep{cole_ribosomal_2009, desantis_greengenes_2006, pruesse_silva_2007}. 
The sequence from an unknown strain can be compared against the sequences in these databases, after very closely related sequences are grouped into Operational Taxonomic Units (OTUs), and infer likely taxonomy, with the assumption that sequences of $>$95\% identity represent the same genus, whereas sequences of $>$97\% identity represent the same species \citep{schloss_introducing_2005}. 
Additionally, NCBI also provides the 23S ribosomal RNA project for Bacteria and Archaea metataxonomics. 

Because this approach is PCR-based, it suffers from the same issues described previously for conventional PCR, requiring primer design. 
Additionally, it must necessarily account for intragenomic variation between 16S gene copies. 
Microbial profiles generated using different primer pairs need independent validation of performance, and the comparison of data sets across V-regions using different databases might be misleading due to differences in nomenclature and varying precisions in classification, and specific but important taxa are not picked up by certain primer pairs (e.g., \textit{Bacteroidetes} is missed using primers 515F-944R) or due to the database used \citep{abellan-schneyder_primer_2021}. 
Furthermore, targeting of 16S variable regions with short-read sequencing platforms cannot achieve the taxonomic resolution afforded by sequencing the entire (~1500 bp) gene \citep{johnson_evaluation_2019}. 
The emergence of third generating sequencing technologies (see \secref{ssec:3rd_gen_seq}) allows for this limitation to be overcome but currently, only a fraction of the databases includes complete 16S rRNA sequences.

While viruses are an integral part of the microbiota, no universal viral marker genes are available to perform such taxonomic assignments. 
Amplification of whole viral genomes is possible and, in 2015, RNA extracted from whole blood, serum, re-suspended swabs and urine, after targeted amplification of the whole viral genome, proved invaluable in the track of the Ebola virus disease epidemic in West Africa, responsible for >11 thousand deaths, allowing for the characterisation of the infectious agent the determination of its evolutionary rate, signatures of host adaptation, identification and monitoring of diagnostic targets and responses to vaccines and treatments \citep{quick_real-time_2016}.
As an alternative, broad scope viral targeted sequence capture (TSC) panels offer depletion of background nucleic acids and improve the recovery of viral reads by targeting coding sequence from a multitude viral genera, such as VirCapSeq-VERT Capture Panel\footnote{\url{https://sequencing.roche.com/content/dam/rochesequence/worldwide/resources/brochure-vircapseq-vert-capture-panel-SEQ1000117.pdf}} but do not guarantee the full recovery of the viral genome, and can present biases towards certain genera \citep{schuele_assessment_2020, wylie_enhanced_2015}. 

\subsubsection{Shotgun Metagenomics} \label{sssec:shotgun_metagenomics}

Shotgun metagenomics can offer relatively unbiased pathogen detection and characterisation. The capacity to detect all potential pathogens — bacteria, viruses, fungi and parasites — in a sample has great potential utility in the diagnosis of infectious disease \citep{chiu_clinical_2019}, potentially able to provide genotyping, antimicrobial resistance and virulence profiling in a single methodological step. This comes with the cost of producing massive amounts of information that require expert handling and processing, as well as capable computational infrastructures \citep{couto_critical_2018, rossen_practical_2018}.

Clinical applications of shotgun metagenomics derive its roots from the use of microarrays (see \secref{ssec:current_standards}), where it was successfully applied in in-depth microbiome analysis of different sites in the human body, it was the emergence of second-generation sequencing technology and its high throughput of genomic data at a competitive price that made the sequencing of all genomic content, DNA and/or RNA) if a clinical sample a viable possibility for diagnostics (see \secref{ssec:2nd_gen_seq}) \citep{miller_basic_2009, palmer_rapid_2006, chiu_clinical_2019}. The first reported case that demonstrated the utility of shotgun metagenomics was in 2014 with the clinical diagnosis of neuroleptospirosis in a 14-year-old immunodeficient and critically ill boy with meningoencephalitis by \cite{wilson_actionable_2014}, prompting appropriate targeted antibiotic treatment and eventual recovery of the patient. In this case, traditional methods, including an invasive brain biopsy, failed to provide answers, until the shotgun sequencing of cerebrospinal fluid identified 475 of 3,063,784 sequence reads (0.016\%) corresponding to leptospira, for which clinical assays were negative due to its very low abundance. Ever since many other reports of successful application of shotgun metagenomics in clinical metagenomics have been reported. but all in edge cases where traditional diagnostic methods have failed or as proof-of-concept \citep{couto_critical_2018, vijayvargiya_application_2019, sanabria_shotgun-metagenomics_2020, hirakata_application_2021}. 

In public health microbiology, shotgun metagenomics combined with transmission network analysis allowed the investigation and quick action on the food supply of the 2013 outbreak of Shiga toxin-producing \textit{Escherichia coli} (STEC) strain O104:H4 from faecal specimens obtained from patients \citep{loman_culture-independent_2013}. A similar approach was followed in the detection of \textit{Salmonella enterica} subsp. \textit{enterica} serovar Heidelberg from faecal samples in two though to be unrelated outbreaks in the United States of America, as well as the \textit{in situ} abundance and level of intrapopulation diversity of the pathogen, and the possibility of co-infections with \textit{Staphylococcus aureus}, overgrowth of commensal \textit{Escherichia coli}, and significant shifts in the gut microbiome during infection relative to reference healthy samples \citep{huang_metagenomics_2017}. More recently, shotgun metagenomic sequencing has evidenced alterations in the gut microbiota of a subset of COVID-19 patients that present the uncommon gastrointestinal (GI) symptoms, shedding a higher understanding of gut–lung axis affecting the progression of COVID-19 \citep{li_microbiome_2021}.

Clinical diagnostic applications have lagged behind research advances. A significant challenge with shotgun metagenomic approach is the large variation in the pathogen load between patient samples, as evidenced in the studies presented. A low pathogen load and  high contamination of host DNA or even the present microbiome may result in enough data to produce the high-resolution subtype needed to distinguish and cluster the cases that were caused by the same outbreak pathogen source, or, extremely, the undetection of the causative agent \citep{carleton_metagenomic_2019, chiu_clinical_2019}. Differential lysis of human host cells followed by degradation of background DNA has proven an effective method to reduce host contamination, but limitations include potential decreased sensitivity for microorganisms without cell walls, such as \textit{Mycoplasma} spp. or parasites; a possible paradoxical increase in exogenous background contamination by use of additional reagent \citep{salter_reagent_2014, oneil_ribosomal_2013, feehery_method_2013}. Additionally, it is often unclear whether a detected microorganism is a contaminant, coloniser or \textit{bona fide} pathogen, and the lack of golden standards remains one of the biggest challenges when applying these methods in clinical microbiology for diagnosis. 

In addition to negative controls, already a common practice in any sequencing assay and in particular in metataxonomics (see \secref{sssec:metataxonomics}), positive controls can be a way to circumvent the lack of golden standards, either through the spike of the samples with a known amount of a specific DNA/RNA or though the sequencing of samples with known composition and abundance. Well-characterised reference standards and controls are needed to ensure shotgun metagenomics assay quality and stability over time \citep{chiu_clinical_2019, mcintyre_comprehensive_2017}. Most available metagenomic reference materials are highly tailored to a specific application. For example, the ZymoBIOMICS Microbial Community Standard\footnote{\url{https://www.zymoresearch.com/collections/zymobiomics-microbial-community-standards}} is the first commercially available standard for microbiomics and metagenomics studies, providing mock a mock community with defined composition and abundance consisting of Gram-positive, Gram-negative and yeast. It is useful to determine the limit of detection of an assay, and the effectiveness and biases of a given protocol. Standards with a more limited spectrum of organisms are also available, such as the National Institute of Standards and Technology (NIST)\footnote{\url{https://www.nist.gov/}} reference materials for mixed microbial DNA detection, which contain only bacteria. Thus, these materials may not apply to untargeted shotgun metagenomics analyses.

\section{The role of bioinformatics} \label{sec:bioinformatics}

As stated previously (see \secref{sec:genomics_approach} and \secref{ssec:metagenomics}), one of the biggest challenges when dealing with genomic, and in particular metagenomic, data is the lack of golden standards. This is also applicable to the bioinformatic analysis, required due to the amount of data produced by genomic sequencing technologies. This is currently one of the bottlenecks in the deployment of sequencing technology in clinical microbiology as there's no standard in how to deal with the increasing amount of data produced in a fit-for-purpose manner \citep{carrico_primer_2018}.

Bioinformatics is an interdisciplinary research field that applies methodologies from computer science, applied mathematics and statistics to the study of biological phenomena\citep{carrico_primer_2018}. With the widespread use and continuous development of sequencing technologies, bioinformatics has become a cornerstone in modern clinical microbiology. 

Major efforts are being made on the standardisation and assessment of software for the analysis of genomic data, both commercial and open-source \cite{angers-loustau_challenges_2018, gruening_recommendations_2019, sczyrba_critical_2017, couto_critical_2018}. 

\subsection{The FASTQ file} \label{ssec:fastq}

In all sequencing technologies (see \secref{ssec:sequencing}), many copies of the source DNA are randomly fragmented and sequenced. To these sequences, we refer to as reads. In the case of second-generation sequencing (see \secref{ssec:2nd_gen_seq}), one or both ends of the fragment can be sequenced. If a fragment is sequenced from one end, we refer to it as single-end sequencing. If a fragment is sequenced on both ends, spanning the entire fragment, it is called paired-end sequencing.

All sequencing technologies, regardless of generation, produce data in the same standard file format: the FASTQ, a text-based format for storing both a biological sequence (usually nucleotide sequence) and its corresponding quality scores \citep{cock_sanger_2010}. Originally developed at the Wellcome Trust Sanger Institute, the FASTQ has emerged as a common file format for sharing sequencing read data (see \ref{fig:figure3}). The FASTQ can be considered as an extension of the ‘FASTA sequence file format’, originally invented by \cite{pearson_improved_1988}, which includes just the sequence information. A FASTQ file normally uses four lines per sequence:

\begin{itemize}
    \item \textbf{Line 1} begins with a '@' character and is followed by a sequence identifier and an optional description;
    \item \textbf{Line 2} is the raw sequence letters;
    \item \textbf{Line 3} begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again;
    \item \textbf{Line 4} encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence.
\end{itemize}

In FASTQ both the sequence letter and quality score are each encoded with a single ASCII character for brevity. The quality of a sequence in a FASTQ file is represented by a quality value Q is an integer mapping of p, where p is the probability that the corresponding base call is incorrect (see Table \ref{tab:phred_error}). This is called the PHRED score \citep{ewing_base-calling_1998} and is defined by the following equation:

\begin{equation}
\centering
\mathbf{Q}\textsc{phred} = - 10 \times \log \mathbf{P}
\end{equation}
 
The PHRED quality scores $\mathbf{Q}$ is defined as a property which is logarithmically related to the base-calling error probability $\mathbf{P}$.

\begin{table}[h!]
\caption{\textbf{PHRED quality scores are logarithmically linked to error probabilities.} A PHRED Score of 20 indicates the likelihood of finding 1 incorrect base call among 100 bases. In other words, the precision of the base call is 99\%. $\mathbf{Q}$ scores are classified as a property that is associated logarithmically with the probabilities of base calling error $\mathbf{P}$.} \label{tab:phred_error}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Phred Quality Score} & \textbf{Probability of incorrect base call} & \textbf{Base call accuracy} \\ \midrule
10 & 1 in 10        & 90\%     \\
20 & 1 in 100       & 99\%     \\
30 & 1 in 1000      & 99.90\%  \\
40 & 1 in 10,000    & 99.99\%  \\
50 & 1 in 100,000   & 100.00\% \\
60 & 1 in 1,000,000 & 100.00\% \\ \bottomrule
\end{tabular}
\end{table}

Since their introduction, PHRED scores have become the \textit{de facto} standard for representing sequencing read base qualities \citep{cock_sanger_2010}. Despite this convention, the encoding of the Phread score can vary when it is translated to its ASCII representation in the FASTQ file format. For example, the Sanger FASTQ files use ASCII 33–126 to encode PHRED qualities from 0 to 93 (i.e. PHRED scores with an ASCII offset of 33). A full list of available encoding is available in \ref{fig:figure6}. 

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 6.png}
\caption{\textbf{Range of FASTQ quality scores andd their corresponding ASCII encoding.} For raw reads, the range of scores will depend on the technology and the base caller used. Starting in Illumina 1.8, the quality scores have returned to the use of the Sanger format (PHRED+33). For processed reads and long accurate reads, scores may be even higher with, For example, quality values of up to 93 observed in reads from PacBio HiFi reads.}
\label{fig:figure6}
\end{figure*}

\subsubsection{FASTQ file simulation} \label{ssec:fastq_sim}

With the lack of golden standards for metagenomic analysis, the use of simulated mock communities, with known composition, abundance and genomic information, provides a ground truth against which evaluations of success can be made. Given their standard structure and adoption, the generation of simulated FASTQ files from a reference, or a set of references, is very straightforward. 

Multiple computational tools for the simulation of sequencing data, particularly for second and third-generation sequencing technologies, have been developed in recent years, which could be used to compare existing and new bioinformatic analytical pipelines. \cite{escalona_comparison_2016} provides a comprehensive assessment of 23 different read-simulation tools,  highlighting their distinct functionality, requirements and potential applications, as well as providing a selection of suggestions for different simulation tools depending on their purpose. For \textit{in silico} genomic and metagenomic sequence generation, a pletora of tools are available for first, second and third-generation reads (see Figure \ref{fig:figure7}).

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 7.png}
\caption{\textbf{Sequence simulators for genomic and metagenomic data.} For first generation sequencing, Metasim (\url{https://github.com/gwcbi/metagenomics_simulation}) and Grider (\url{https://sourceforge.net/projects/biogrinder/}) can generate mock genomic and metagenomic data, with and without error models respectively. For Illumina data, ART (\url{https://www.niehs.nih.gov/research/resources/software/biostatistics/art/index.cfm}), InSilicoSeq (\url{https://github.com/HadrienG/InSilicoSeq}) and CAMISIM (\url{https://github.com/CAMI-challenge/CAMISIM}) represent options for in silico data generation. Due to their differences, the third generation Pacific BioSciences (PacBio) and Oxford Nanopore (ONT) have distict software for in silico data generation. The first can be accomplished by LongISLND (\url{https://bioinform.github.io/longislnd/}) and PBSIM2 (\url{https://github.com/yukiteruono/pbsim2}) got genomic data, and SimLORD (\url{https://bitbucket.org/genomeinformatics/simlord/src}) fot metagenomic data, with and without error model. The latter BadRead (\url{https://github.com/rrwick/Badread}) and NanoSim (\url{https://github.com/bcgsc/NanoSim}) can genenrate genomic and metagenomic \textit{in silico} data, with and withouth error model. Additionally, for genomic data, LongISLND and SiLiCO (\url{https://github.com/ethanagb/SiLiCO}) generate data with and without error, respectively. Adapted from \cite{escalona_comparison_2016}.}
\label{fig:figure7}
\end{figure*}

\subsubsection{FASTQ quality assessment and quality control} \label{ssec:fastq_quality}

Quality assessment and control is a basal step to any analysis, and aims to (1) remove and/or filter low quality and low complexity reads, (2) trim adapters, and (3) remove host sequences from the samples’ raw data. There are many tools available but the most commonly used are FastQC\footnote{\url{https://www.bioinformatics.babraham.ac.uk/projects/fastqc/}} (Babraham Bioinformatics) for quality control, followed by Trimmomatic \citep{bolger_trimmomatic_2014}, Cutadapt \citep{martin_cutadapt_2011} or fastp \citep{chen_fastp_2018} to trim and/or filter adaptors, low quality and low complexity sequences. For long-read sequencing, tools like NanoPlot and NanoStats \citep{de_coster_nanopack_2018}, and Filtlong\footnote{\url{https://github.com/rrwick/Filtlong/}} can perform the equivalent quality assessment and control, adapter trimming and low quality trimming, respectively. 

\subsection{Direct taxonomic assignment and characterisation} \label{ssec:taxonomic_assignment}

A piece of important information that can be retrieved directly from the quality-controlled read data: (1) the identification and characterisation of the microbes present in a sample and (2) their relative abundance. Taxonomic classification methods can vary depending on the sequencing methodology used: pure culture, metataxonomics and amplicon metagenomics, and shotgun metagenomics.

From pure culture, taxonomic identification of the read content of a sample is useful to assess contamination. Tools like Kraken2 \citep{wood_kraken_2014, wood_improved_2019} and Braken \citep{lu_bracken_2017}. These tools, relying on a database, assign taxonomic labels to reads and are therefore biased to the contents of the database used. Various databases are available\footnote{\url{https://benlangmead.github.io/aws-indexes/k2}}, varying in size and content (archaea, bacteria, viral, plasmid, human and eukaryotic pathogens), and therefore in sensitivity depending on the resources available and the purpose intended. Alternatively, there are options to create custom databases.  

These tools are also extremely useful to assess the contents of a metagenomic sample. Alternatives such as Midas \citep{nayfach_integrated_2016}, Kaiju, \citep{menzel_fast_2016}, and MetaPhlAn2 \citep{truong_metaphlan2_2015} offer the same analysis as Kraken and Bracken using different algorithms, and with the disadvantage that they come prepackaged with their own databases, without the option to create a tailored database, limiting their applicability. Kaiju differs from the other tools by using a protein reference database, instead of nucleotide, but no pre-built version is available, requiring significant resources to build and index the database pre-use. The long-read data of third-generation sequencing technologies (see \secref{ssec:3rd_gen_seq}) can be treated as single-end reads, and all tools mentioned accommodate the classification of single-end files. 

\subsection{From reads to genomes} \label{ssec:reads_2_genomes}

Due to the limitations of current sequencing technologies (see \secref{ssec:sequencing}), the order of the reads produced by these machines cannot be preserved. 
Therefore, to obtain the true original genomic sequence the process of "genome assembly" has to occur. 
The term "draft genome" is commonly used because these sequencing technologies do not generate a single closed genome, particularly short-read such as in second generation sequencing (see \secref{ssec:2nd_gen_seq}) which need to be assembled into usually a series of sequences (contigs) that may cover up to 95\% to 99\% of the strain genome \citep{carrico_primer_2018}. 
Long-read technologies (see \secref{ssec:3rd_gen_seq}) allow for this value to reach 100\%, effectively producing closed, complete genomes, notwithstanding that this value can sometimes overcome the 100\% due to overlap \citep{wick_benchmarking_2021}. 

Assembling reads into contigs has many advantages, namely that longer sequences are more informative, allowing the consideration of whole genes or even gene clusters within a genome and to understand larger genetic variants and repeats. Additionally, it has the effect of removing most sequencing errors, though this can be at the expense of new assembly errors \citep{ayling_new_2020}. Two methods are used to obtain draft genomes: (1) through reference-guided sequence assembly, or (2), through \textit{de novo} sequence assembly.

\subsubsection{Genomes through reference-guided sequence assembly}

A reference-guided genome assembly uses an already sequenced reference genome to assemble a new genome, making use of the similarity between target and reference species to gain additional information, which often lead to a more complete and improved genome \citep{rausch_consistency-based_2009, lischer_reference-guided_2017}. 
This process is usually done through the mapping of the reads to a closely related reference sequence, and as more and more species get sequenced, the chance that a genome of the same or related species is already available, in which a significant proportion of the reads can be mapped, increase greatly. 
This process usually includes the following steps: (1) the reference genome has to be indexed, allowing compression of the input text while still permitting fast sub-string queries, (2) for each short-read several sub sequences (seeds) are taken and searched to find their exact matches in the reference  (candidate regions), (3) each short-read is then aligned to all corresponding candidate regions, and (4) the consensus sequence is computed in which the reference sequence is corrected when there is enough evidence of an difference based on the mapped
reads, identifying the differences between it and the newly generated consensus sequence \citep{bayat_methods_2020}.
Besides variants, the new consensus genome might have insertions or deletions with respect to the reference genome.

Besides the generation of a consensus sequence, the mapping of the reads to the reference sequence can be used to estimate sequence depth and breadth of coverage. 
Depth of coverage, often referred to simply as coverage, refers to the average number of times each nucleotide position in the strain's genome has a read that aligns to that position. Depending on the study goals, bacterial species and the intended analyses, the optimal depth of coverage varies. 
In public repositories, most submissions have a depth of coverage ranging from 15 to 500 times \citep{carrico_primer_2018}. 
Breadth of coverage is defined as the ratio of covered sequence on the reference by the aligned reads.

\subsubsection{Genomes through \textit{de novo} sequence assembly}

De novo assembly refers to the bioinformatics process whereby reads are assembled into a draft genome using only the sequence information of the reads. Two methods are used to obtain draft genomes without the need of a reference genome: (1) through Overlap, Layout and Consensus, or (2) De Bruijn graph assembly (see Figure \ref{fig:figure8}). The \textit{de novo} assembly methods provide longer sequences that are more informative than shorter sequencing data and can provide a more complete picture of the microbial community in a given sample.

\begin{figure*}[h!]
\centering
\includegraphics[width=\textwidth]{figures/introduction/Figure 8.png}
\caption{\textbf{Approaches to \textit{de novo} genome assemble.} In Overlap, Layout, Consensus assembly, (1) overlaps are found between reads and an overlap graph constructed (edges indicate overlapping reads). (2) Reads are laid out into contigs based on the overlaps (lines indicate overlapping portions). (3) The most likely sequence is chosen to construct consensus sequence. In De Bruijn graph assembly, (1) reads are decomposed into kmers of a determined size by sliding a window of size k (in here of k=3) across the reads. (2) The kmers become vertices in the De Bruijn graph, with edges connecting overlapping kmers. Polymorphisms (red) form branches in the graph. A count is kept of how many times a kmer is seen, shown here as numbers above kmers. (3) Contigs are built by walking the graph from edge nodes. A variety of heuristics handle branches in the graphs—for example, low coverage paths, as shown here, may be ignored. Adapted from \cite{ayling_new_2020}}.
\label{fig:figure8}
\end{figure*}

\paragraph{Overlap, Layout and Consensus assembly} \label{sssec:OLC_assembly} \mbox\\

First generation sequencing technology (see \secref{ssec:1st_gen_seq}) produces far fewer reads than second generation sequencing technology (see \secref{ssec:2nd_gen_seq}, but individual reads are longer (500–1000 bp). 
Assembly of Sanger data usually uses overlap-layout consensus (OLC) approaches, in which:

\begin{itemize}
    \item Overlaps are computed by comparing all reads to all other reads;
    \item Overlaps are grouped together to form contigs;
    \item A consensus contiguous sequence, or contig, is determined by picking the most likely nucleotides from the overlapping reads.
\end{itemize}

These type of assemblers were very popular in the early 2010s, with assemblers such as Celera\footnote{\url{https://www.cbcb.umd.edu/software/celera-assembler}},  Genovo\footnote{\url{https://cs.stanford.edu/genovo }}, xGenovo\footnote{\url{http://xgenovo.dna.bio.keio.ac.jp/}} and BBAP\footnote{\url{http://homepage.ntu.edu.tw/~youylin/BBAP.html}} having been widely used \citep{myers_whole-genome_2000, hutchison_genovo_2010, afiahayati_extended_2013, lin_novo_2017}.
With the emergence of third-generation sequencing (see \secref{ssec:3rd_gen_seq}), OLC assemblers have been increasingly developed and adopted by the community to assembly long-read data. 
In the latest years, ra\footnote{\url{https://github.com/lbcb-sci/ra}}, raven\footnote{\url{https://github.com/lbcb-sci/raven}} and canu\footnote{\url{https://github.com/marbl/canu}}, the latter being a a fork of the Celera Assembler, have become staples in the community, showing good reliability and amassing over 3000 citations \citep{vaser_yet_2019, koren_canu_2017, wick_benchmarking_2021}.  %missing raven

\paragraph{De Bruijn graph assembly} \label{sssec:dbg_assembly} \mbox{}\\

In the De Bruijn assembly graph, reads are split into overlapping k-mers where nodes of the graph represent k-mers where:

\begin{itemize}
    \item A directed edge from node N\textsubscript{a} to node N\textsubscript{b} indicates that N\textsubscript{b} is next to N\textsubscript{a} in a read;
    \item The number of nodes in the De Bruijn graph is theoretically the total number of identical k-mers in the genome;
    \item The weight on the edge indicates the number of times  N\textsubscript{b} is observed next to  N\textsubscript{a} in all reads.
\end{itemize}

Thus, the weight of an edge indicates the possibility that two k-mers appear after each other in the DNA sequence. A path in the graph where all edges have the highest weight is the most likely to be a part of the genome \citep{bayat_methods_2020}.

Most second-generation sequencing (see \secref{ssec:2nd_gen_seq} assemblers, such as SPAdes\footnote{\url{https://github.com/ablab/spades/}}, SKESA\footnote{\url{https://github.com/ncbi/SKESA/}} and MEGAHIT\footnote{\url{https://github.com/voutcn/megahit/}}, use a multiple k-mer De Bruijn graph, starting with the lowest size and iteratively adding k-mers of increasing length to connect the graph \citep{bankevich_spades_2012, souvorov_skesa_2018, li_megahit_2015}. Older assemblers, such as Velvet\footnote{\url{https://www.ebi.ac.uk/~zerbino/velvet/}}, Ray\footnote{\url{https://sourceforge.net/projects/denovoassembler/f}} and SoapDeNovo2\footnote{\url{https://sourceforge.net/projects/soapdenovo2/}} use a single k-mer strategy for the De Bruijn graph construction \citep{zerbino_velvet_2008, boisvert_ray_2010, luo_soapdenovo2_2012}. 

\subsubsection{Assembly quality assessment and quality control} \label{ssec:assembly_quality}

The success of an assembly is evaluated in two steps: (1) globally, through intrinsic characteristics of the assembly itself, and (2) relative to a reference genome. The computation of the global metrics is performed through statistics inherent to the complete set of contigs assembled per sample, independent of the species/sample of origin. Commonly, these statistics include information on contig number, its median size and number ambiguous bases. The comparison with a reference sequence allows for statistics such as number of misassemblies, meaning contigs that do not reflect the structural organisation in the reference sequence, to be computed. 

The assessment and evaluation of genome assemblies has been a relevant field ever since the emergence of the assembly process itself. 
The most widely adopted is QUAST\footnote{\url{http://quast.sourceforge.net/quast}}, can evaluate assemblies both with a reference genome, as well as without a reference, producing many reports, summary tables and plots to help compare and assess assembly success \citep{gurevich_quast_2013}. 

\subsection{Reproducibility and transparency} \label{ssec:reproducibility}

Several steps that can be implemented to ensure the transparency and reproducibility of the chosen bioinformatic workflow. 
Favouring open-source tools, with clear documentation describing the methodology implemented, and stating the version of the software used and which parameters were used enables the comparison of results. 
This can be simplified by containerising all the software tools with one of the many solutions available, like Docker\footnote{\url{https://www.docker.com/}} or Singularity\footnote{\url{https://sylabs.io/}} \citep{kurtzer_singularity_2017}. 

The use of workflow managers, like nextflow\footnote{\url{https://www.nextflow.io/}}, snakemake\footnote{\url{https://snakemake.github.io/}} or the Galaxy Project\footnote{\url{https://galaxyproject.org/}}, will push reproducibility to the next level by taking advantage of the containerisation and scalability, enabling the workflow to be executed with the same parameters in the same conditions in a multitude of different environments \citep{di_tommaso_nextflow_2017, molder_sustainable_2021, afgan_galaxy_2018}. 


\section{Bioinformatic Analysis for Metagenomics} \label{sec:metagenomics_bioinfo}

As mentioned previously (see \secref{ssec:metagenomics}, Metagenomic shotgun sequencing circumvents the need for cultivation and, compared with metataxonomics, avoids biases from primer choice, enables the detection of organisms across all domains of life and \textit{de novo} assembly of genomes and functional genome analyses. However, highly uneven sequencing depth of different organisms and low depth of coverage per species are drawbacks that limit taxa

\subsubsection{Metataxonomics} \label{ssec:metataxonomics_bioinfo}

Metataxonomics (see \secref{sssec:metataxonomics}) is the most widely used technique for microbial diversity analysis \citep{hilton_metataxonomic_2016}, and due to its particularities, the analysis of this data is also very particular. Data analyses are mostly carried out through specialised pipelines that wrap and combine several tools, offering the possibility to follow a simple protocol with default configurations or choose between a plethora of different configurations to adjust for any particular needs. Quantitative Insights Into Microbial Ecology 2 (QUIIME2)\footnote{\url{https://qiime2.org/}} \citep{bolyen_reproducible_2019} has become the \textit{de facto} tool for metataxonomic analysis as a framework with an ever-growing suite of plugins and intuitive data visualisation tools for the assessment of results. Mothur \citep{schloss_introducing_2009} and UPARSE \citep{edgar_uparse_2013} are also a popular alternative although resulting outputs differing significantly between pipelines despite using the same inputs having been reported by \cite{marizzoni_comparison_2020}, with a magnitude that is comparable to differences in upstream sample treatment and sequencing procedures.  A typical workflow starts with quality filtering, error correction and removal of chimeric sequences. These quality control steps are followed by either taxonomic assignment of reads or a clustering step where reads are gathered into OTUs given their sequence identity, followed by statistical analysis to assess differences between given groups. Taxonomic assignment methods classify query sequences based on the best hit found in reference databases of annotated sequences, being heavily dependent on the completeness of the reference databases (see \secref{sssec:metataxonomics}). Classification is further limited by lack of species annotation in most reference databases \citep{westcott_novo_2015}. Alternatively, the same approach of direct taxonomic classification, without OTU clustering, can be followed as with genomic and shotgun metagenomic data, given that the databases include rRNA sequences.

OTU clustering methods can be categorised into: (1) computationally expensive hierarchical methods that cluster sequences based on a distance matrix measuring the difference between each pair of sequences, (2) less expensive heuristic methods cluster sequences into OTUs based on a pre-defined threshold, generally, with a sequence being selected as a seed and the rest of the sequences being analysed sequentially and added to existing or new clusters according to the defined threshold, and (3) model based clustering methods that do not rely on a pre-defined and fixed threshold, defining OTUs based on a soft threshold and carrying out the clustering process based on methods such as an unsupervised probabilistic Bayesian clustering algorithm \citep{hao_clustering_2011}. These methods offer the possibility to cluster sequences based on criteria that do not depend on reference databases and are especially useful in less characterised microbial communities or with a high representation of uncultured microbes. Due to the assumptions made with this strategy, it is sensitive to under or overestimation of the number of OTUs in a sample as defining a threshold to accurately cluster sequences is difficult \citep{westcott_novo_2015}.

\subsubsection{Shotgun metagenomics} \label{ssec:shotgun_metagenomics_bioinfo}

A plethora of open-source tools are available specifically for shotgun metagenomic data, and several combinations of these tools can be used to characterise the causative agent in a patient's infection in a fraction of the time required by the traditional methods. 

A major additional difficulty of shotgun metagenomic data is the overpowering quantities of host DNA that are often sequenced, making the microbial community sometimes close to undetectable \citep{couto_critical_2018}.
The presence of contaminants, from the bench process to the pre-existing biota, and the cost associated with this methodology, are also major hinders in its applicability in the clinic.
They account for major caveats and must be made aware of when analysing the data.

\begin{figure*}[h!]
\centering
\includegraphics[]{figures/introduction/Figure9.pdf}
\caption{Typical bioinformatic analysis procedure for metagenomic data}
\label{fig:figure9}
\end{figure*}

The basic strategies for analysing shotgun metagenomic data can be simplified in the scheme in Figure \ref{fig:figure9}. 
One of the biggest challenges when doing metagenomic analysis is differentiating between colonisation and infection by successfully discriminating between a potential pathogen and background microbiota. 
In the latter, when analysing samples from presumably sterile sites, like Cerebrospinal fluid (CSF) and blood, it is safe to assume that all organisms found are of interest. 
In locations with a microbiota, the inclusion of negative controls is essential for the correct identification of contaminants in the taxonomic results, whether originated from the sample collection, handling or sequencing process. 
The use of spiked metagenomic samples as positive control might guide the detection of the possible pathogens by comparing relative abundance between the samples. 
These controls should be processed similarly to the samples and the taxonomic results should be filtered out from the final report. 

As explored in \secref{ssec:reads_2_genomes}, longer sequences are more informative than shorter sequencing data, as the one obtained from second-generation sequencing (see \secref{ssec:2nd_gen_seq}), and can provide a more complete picture of the microbial community in a given samples. 
Several dedicated metagenomic assembly tools are available, such as metaSPAdes\footnote{\url{https://github.com/ablab/spades/}}  and MegaHIT\footnote{\url{https://github.com/voutcn/megahit/}} \citep{nurk_metaspades_2017, li_megahit_2015}. 
These tools, in comparison to single-cell data assemblers, are better at dealing with the combination of intra and intergenomic repeats and uneven sequencing coverage \citep{olson_metagenomic_2017}.
For third-generation sequencing, dedicated metagenomic assemblers have recently emerged, such as meta-flye\footnote{\url{https://github.com/fenderglass/Flye/}} which expands on the original flye assembler by overcoming a k-mer selection limitation on low abundance species \citep{kolmogorov_metaflye_2020}. Nevertheless, the use of non dedicated assemblers for metagenomics may come with the cost of wrongly interpret variation as error, especially in samples that contained closely related species and the construction of chimeric sequences as traditional assemblers follow the basic principle that the coverage in a sample is constant \citep{teeling_current_2012}. 

The assembly-based approach requires the grouping of the different contigs into bins, ideally each collecting the sequences that belong to a microorganism present in the sample. The binning process can be taxonomy dependent, relying on a database to aggregate the sequences, or independent. The independent approach has the benefit of not relying on a database, but instead it uses the composition of each sequence and coverage profiles to cluster together sequences that might belong to the same organism. These algorithms don’t require prior knowledge about the genomes in a given sample, instead relying on features inherent to the sequences in the sample. Although most binning software can work with single metagenomic samples, most make use of differential coverage of multiple samples to improve the binning process \citep{sedlar_bioinformatics_2017}. It allows the handling of complex ecosystems and might be crucial when analysing samples recovered from sites with a complex microbiota. A comparison of five taxonomic independent  and four taxonomic binning software by \cite{sczyrba_critical_2017} revealed that, for taxonomic independent approaches, MaxBin2\footnote{\url{https://sourceforge.net/projects/maxbin2/}} had the highest completeness and purity in the bins obtained \citep{}. For taxonomic binning, working similarly to the direct taxonomic assignment of the sequencing data, PhyloPythiaS+\footnote{\url{https://github.com/algbioi/ppsp}} obtained better results in accuracy, completeness and purity, followed by Kraken\footnote{\url{https://github.com/DerrickWood/kraken2/}} that still obtained decent results with the added benefit of very high speed of analysis, ease of use and inclusion of the pre-built databases \citep{gregor_phylopythias_2016, wood_kraken_2014}.

The last step on the assembly methodology is the evaluation of the completeness and contamination of the bins. When using a taxonomic binner, the effects of contamination are mitigated as the sequence clustering is performed based on matches with reference database. The contaminants, if present in the database, will be separated into different bins or just added to the bin of unclassified sequences. When using a taxonomic independent binning software, the composition and abundance might not be enough to discriminate between all the organisms, with the possible result of having bins with contaminating sequences of other organisms present in the sample. CheckM\footnote{\url{https://ecogenomics.github.io/CheckM/}} assesses the quality of the recovered genomes, estimating completeness and contamination by evaluating ubiquitous single-copy genes \citep{parks_checkm_2015}.
 
Another problem with metagenomic assembly is the high number of ambiguities that fail to being resolved, mostly due to the possible presence of several strains of the same species or species that are closely related. When faced with this ambiguities the assembler usually breaks the sequence, leading to fragmented reconstructions of genomes. MetaQUAST\footnote{\url{https://github.com/ablab/quast}} that besides computing several metrics to evaluate assembly quality like number of contigs, maximum contig length, etc, also uses reference-based method, either provided by the user or by identifying the appropriate reference sequences by 16S ribosomal RNA identification, to identify mis-assemblies and structural variants \citep{mikheenko_metaquast_2016}. All downstream processes used in whole genome sequencing draft genomes can be applied to each of the resulting binned genomes, that now represent a taxonomic unit recovered from the original metagenomic sample. 
